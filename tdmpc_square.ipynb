{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "229d1070-9828-4b02-8e37-bb8d01af7b75",
      "metadata": {
        "id": "229d1070-9828-4b02-8e37-bb8d01af7b75"
      },
      "source": [
        "# 2024年 世界モデル コンペティション 参考notebook  \n",
        "\n",
        "第8回演習で利用したDreamerに修正を加え，Dreamer v2を用いたベースラインコードになっています．  \n",
        "こちらを動かしていただけば，提出時にエラーが発生しない結果を得ることができます（参考用としてcolabの無料枠で1時間ほどで終わるようにパラメータを変えているため，性能は出ないです）．  \n",
        "\n",
        "**目次**\n",
        "1. [準備](#scrollTo=b986f379-97f5-4449-b4c6-7cc385d1f474)\n",
        "2. [環境の設定](#scrollTo=c7819663-fffc-44e5-842f-779564dd8227)\n",
        "3. [補助機能の実装](#scrollTo=6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae)\n",
        "4. [モデルの実装](#scrollTo=0662612e-701b-41a2-8679-25ad03fef367)\n",
        "5. [学習](#scrollTo=b06c188f-8a87-42e7-9f61-7f385eccc565)\n",
        "6. [モデルの保存](#scrollTo=aa693a51-a4cb-4ad4-be2b-322cbd68443d)\n",
        "7. [学習済みパラメータで評価](#scrollTo=c4b31352-bafa-46ed-8bcc-632a24dfced6)\n",
        "\n",
        "以下良い性能を出すためにできる工夫の例です．  \n",
        "- ハイパーパラメータを調整する．  \n",
        "  - バッチサイズを大きくする．\n",
        "  - 更新回数を増やす（update_freqを小さくする）．\n",
        "  - モデルの次元数を大きくする．  など\n",
        "- Dreamer v2の各モデルのアーキテクチャを変更する．\n",
        "- Dreamer v2以外の学習手法を用いる．"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b986f379-97f5-4449-b4c6-7cc385d1f474",
      "metadata": {
        "id": "b986f379-97f5-4449-b4c6-7cc385d1f474"
      },
      "source": [
        "## 1. 準備  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3d2823-ec95-4607-a33f-37bfe410f6b4",
      "metadata": {
        "id": "1c3d2823-ec95-4607-a33f-37bfe410f6b4"
      },
      "source": [
        "必要なライブラリのインストール．各自必要なライブラリがある場合は追加でインストールしてください．  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5b9qtVaIMAB-",
      "metadata": {
        "id": "5b9qtVaIMAB-"
      },
      "outputs": [],
      "source": [
        "# !pip install gym==0.26.2 gym[atari]==0.26.2 gym[accept-rom-license]==0.26.2 autorom ale-py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfce4b3",
      "metadata": {},
      "source": [
        "ローカルで動かすならこっち\n",
        "```bash\n",
        "uv sync\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b7e6f3-880d-4936-a9ea-786d8051c8bf",
      "metadata": {
        "id": "c9b7e6f3-880d-4936-a9ea-786d8051c8bf"
      },
      "source": [
        "### 1.1 ライブラリインポート  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f380bb8a-0a49-43b0-9894-f303670bae41",
      "metadata": {
        "id": "f380bb8a-0a49-43b0-9894-f303670bae41",
        "jupyter": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from time import time\n",
        "import os\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from gym.wrappers import ResizeObservation\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from functorch import combine_state_for_ensemble\n",
        "from tensordict.tensordict import TensorDict\n",
        "from torchrl.data.replay_buffers import ReplayBuffer, LazyTensorStorage\n",
        "from torchrl.data.replay_buffers.samplers import SliceSampler\n",
        "import re\n",
        "import sys\n",
        "import math\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e26ffcc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# エラー回避\n",
        "from ale_py.env import gym as ale_gym\n",
        "from typing import Any, Text\n",
        "\n",
        "# Patch to allow rendering Atari games.\n",
        "# The AtariEnv's render method expects the mode to be in self._render_mode\n",
        "# (usually initialized with env.make) instead of taking mode as a param.\n",
        "_original_atari_render = ale_gym.AtariEnv.render\n",
        "\n",
        "\n",
        "def atari_render(self, mode: Text = 'rgb_array') -> Any:\n",
        "  original_render_mode = self._render_mode\n",
        "  try:\n",
        "    self._render_mode = mode\n",
        "    return _original_atari_render(self)\n",
        "  finally:\n",
        "    self._render_mode = original_render_mode\n",
        "\n",
        "\n",
        "ale_gym.AtariEnv.render = atari_render"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7819663-fffc-44e5-842f-779564dd8227",
      "metadata": {
        "id": "c7819663-fffc-44e5-842f-779564dd8227"
      },
      "source": [
        "## 2. 環境の設定  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd206f5-6f74-49ae-998f-7598564cdb6e",
      "metadata": {
        "id": "0dd206f5-6f74-49ae-998f-7598564cdb6e"
      },
      "source": [
        "### 2.1 Repeat Action  \n",
        "- こちらで実装している環境を用いてOmnicampus上では評価を行います．  \n",
        "- モデルによって変更する可能性があると想定している部分は以下のとおりです．\n",
        "    - 画像のレンダリングサイズ(ResizeObervationクラスのshape)．\n",
        "    - 同じ行動を繰り返す数（RepeatActionクラスのskip）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "976179d0-2365-4440-bd7c-7e07ae218901",
      "metadata": {
        "id": "976179d0-2365-4440-bd7c-7e07ae218901"
      },
      "outputs": [],
      "source": [
        "# define the environment wrapper\n",
        "class RepeatAction(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    同じ行動を指定された回数自動的に繰り返すラッパー. 観測は最後の行動に対応するものになる\n",
        "    \"\"\"\n",
        "    def __init__(self, env, skip=4, max_steps=100_000):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.max_steps = max_steps if max_steps else float(\"inf\")  # イテレーションの制限\n",
        "        self.steps = 0  # イテレーション回数のカウント\n",
        "        self.height = env.observation_space.shape[0]\n",
        "        self.width = env.observation_space.shape[1]\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(3, self.height, self.width),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        # obs[0]が[64,64,3]の場合、[3,64,64]に変換\n",
        "        return np.transpose(obs[0], (2, 0, 1))\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.steps >= self.max_steps:  # 100kに達したら何も返さないようにする\n",
        "            print(\"Reached max iterations.\")\n",
        "            return None\n",
        "\n",
        "        total_reward = 0.0\n",
        "        self.steps += 1\n",
        "        for _ in range(self._skip):\n",
        "            # ここでaは離散である必要がある．\n",
        "            discrete_a = torch.argmax(action).cpu()\n",
        "            obs, reward, done, truncated, info = self.env.step(discrete_a)\n",
        "\n",
        "            total_reward += reward\n",
        "            if self.steps >= self.max_steps:  # 100kに達したら終端にする\n",
        "                done = True\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # obsが[64,64,3]の場合、[3,64,64]に変換して返す\n",
        "        obs_transposed = np.transpose(obs, (2, 0, 1))\n",
        "        return obs_transposed, total_reward, done, truncated, info\n",
        "    def rand_act(self):\n",
        "        # -1から1の間のサイズが[9]のtorch.tensorを返す\n",
        "        return torch.rand(9) * 2 - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8affc9bc-eabb-49b1-b4ba-24731e960e32",
      "metadata": {
        "id": "8affc9bc-eabb-49b1-b4ba-24731e960e32"
      },
      "outputs": [],
      "source": [
        "# define make_env\n",
        "def make_env(seed=None, img_size=64, max_steps=100_000):\n",
        "    env = gym.make(\"ALE/MsPacman-v5\")\n",
        "\n",
        "    # シード固定\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "\n",
        "    env = ResizeObservation(env, (img_size, img_size))\n",
        "    env = RepeatAction(env=env, skip=4, max_steps=max_steps)\n",
        "\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae",
      "metadata": {
        "id": "6b9cdd13-ce4a-44b4-a01d-5a19d4e38bae"
      },
      "source": [
        "## 3. 補助機能の実装  \n",
        "- モデルを保存する際に利用できるクラス，torchのシード値を固定できる関数です．   \n",
        "- 提出いただくパラメータの保存や読み込みにこちらのクラスを必ず利用する必要はありません  ．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "439e4333-d460-4ef2-9aab-5c6f673f8261",
      "metadata": {
        "id": "439e4333-d460-4ef2-9aab-5c6f673f8261"
      },
      "outputs": [],
      "source": [
        "# モデルパラメータをGoogleDriveに保存・後で読み込みするためのヘルパークラス\n",
        "class TrainedModels:\n",
        "    def __init__(self, *models) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        models : nn.Module\n",
        "            保存するモデル．複数モデルを渡すことができます．\n",
        "\n",
        "        使用例: trained_models = TraindModels(encoder, rssm, value_model, action_model)\n",
        "        \"\"\"\n",
        "        assert np.all([nn.Module in model.__class__.__bases__ for model in models]), \"Arguments for TrainedModels need to be nn models.\"\n",
        "\n",
        "        self.models = models\n",
        "\n",
        "    def save(self, dir: str) -> None:\n",
        "        \"\"\"\n",
        "        initで渡したモデルのパラメータを保存します．\n",
        "        パラメータのファイル名は01.pt, 02.pt, ... のように連番になっています．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : str\n",
        "            パラメータの保存先．\n",
        "        \"\"\"\n",
        "        for i, model in enumerate(self.models):\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\")\n",
        "            )\n",
        "\n",
        "    def load(self, dir: str, device: str) -> None:\n",
        "        \"\"\"\n",
        "        initで渡したモデルのパラメータを読み込みます．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir : str\n",
        "            パラメータの保存先．\n",
        "        device : str\n",
        "            モデルをどのデバイス(CPU or GPU)に載せるかの設定．\n",
        "        \"\"\"\n",
        "        for i, model in enumerate(self.models):\n",
        "            model.load_state_dict(\n",
        "                torch.load(\n",
        "                    os.path.join(dir, f\"{str(i + 1).zfill(2)}.pt\"),\n",
        "                    map_location=device\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4880f046-6076-4eea-a0f7-6ef16e875ab7",
      "metadata": {
        "id": "4880f046-6076-4eea-a0f7-6ef16e875ab7"
      },
      "outputs": [],
      "source": [
        "# set_seed\n",
        "def set_seed(seed: int) -> None:\n",
        "    \"\"\"\n",
        "    Pytorch, NumPyのシード値を固定します．これによりモデル学習の再現性を担保できます．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        シード値．\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0662612e-701b-41a2-8679-25ad03fef367",
      "metadata": {
        "id": "0662612e-701b-41a2-8679-25ad03fef367"
      },
      "source": [
        "## 4. モデルの実装\n",
        "\n",
        "https://github.com/DarthUtopian/tdmpc_square_public/tree/main/tdmpc_square/tdmpc_square\n",
        "\n",
        "TD-M(PC)^2を実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ee03fe13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# init\n",
        "def weight_init(m):\n",
        "    \"\"\"Custom weight initialization for TD-MPC2.\"\"\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Embedding):\n",
        "        nn.init.uniform_(m.weight, -0.02, 0.02)\n",
        "    elif isinstance(m, nn.ParameterList):\n",
        "        for i, p in enumerate(m):\n",
        "            if p.dim() == 3:  # Linear\n",
        "                nn.init.trunc_normal_(p, std=0.02)  # Weight\n",
        "                nn.init.constant_(m[i + 1], 0)  # Bias\n",
        "\n",
        "\n",
        "def zero_(params):\n",
        "    \"\"\"Initialize parameters to zero.\"\"\"\n",
        "    for p in params:\n",
        "        p.data.fill_(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c9499196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# layers\n",
        "class Ensemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Vectorized ensemble of modules.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modules, **kwargs):\n",
        "        super().__init__()\n",
        "        modules = nn.ModuleList(modules)\n",
        "        fn, params, _ = combine_state_for_ensemble(modules)\n",
        "        self.vmap = torch.vmap(\n",
        "            fn, in_dims=(0, 0, None), randomness=\"different\", **kwargs\n",
        "        )\n",
        "        self.params = nn.ParameterList([nn.Parameter(p) for p in params])\n",
        "        self._repr = str(modules)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.vmap([p for p in self.params], (), *args, **kwargs)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Vectorized \" + self._repr\n",
        "\n",
        "\n",
        "class ShiftAug(nn.Module):\n",
        "    \"\"\"\n",
        "    Random shift image augmentation.\n",
        "    Adapted from https://github.com/facebookresearch/drqv2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pad=3):\n",
        "        super().__init__()\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        # batch化せずにxが入力されることがある\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.unsqueeze(0) # [3, 64, 64] -> [1, 3, 64, 64]\n",
        "        n, _, h, w = x.size()\n",
        "        assert h == w\n",
        "        padding = tuple([self.pad] * 4)\n",
        "        x = F.pad(x, padding, \"replicate\")\n",
        "        eps = 1.0 / (h + 2 * self.pad)\n",
        "        arange = torch.linspace(\n",
        "            -1.0 + eps, 1.0 - eps, h + 2 * self.pad, device=x.device, dtype=x.dtype\n",
        "        )[:h]\n",
        "        arange = arange.unsqueeze(0).repeat(h, 1).unsqueeze(2)\n",
        "        base_grid = torch.cat([arange, arange.transpose(1, 0)], dim=2)\n",
        "        base_grid = base_grid.unsqueeze(0).repeat(n, 1, 1, 1)\n",
        "        shift = torch.randint(\n",
        "            0, 2 * self.pad + 1, size=(n, 1, 1, 2), device=x.device, dtype=x.dtype\n",
        "        )\n",
        "        shift *= 2.0 / (h + 2 * self.pad)\n",
        "        grid = base_grid + shift\n",
        "        return F.grid_sample(x, grid, padding_mode=\"zeros\", align_corners=False)\n",
        "\n",
        "\n",
        "class PixelPreprocess(nn.Module):\n",
        "    \"\"\"\n",
        "    Normalizes pixel observations to [-0.5, 0.5].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.div_(255.0).sub_(0.5)\n",
        "\n",
        "\n",
        "class SimNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplicial normalization.\n",
        "    Adapted from https://arxiv.org/abs/2204.00616.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.dim = cfg.simnorm_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        shp = x.shape\n",
        "        x = x.view(*shp[:-1], -1, self.dim)\n",
        "        x = F.softmax(x, dim=-1)\n",
        "        return x.view(*shp)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SimNorm(dim={self.dim})\"\n",
        "\n",
        "\n",
        "class NormedLinear(nn.Linear):\n",
        "    \"\"\"\n",
        "    Linear layer with LayerNorm, activation, and optionally dropout.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, dropout=0.0, act=nn.Mish(inplace=True), **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.ln = nn.LayerNorm(self.out_features)\n",
        "        self.act = act\n",
        "        self.dropout = nn.Dropout(dropout, inplace=True) if dropout else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        return self.act(self.ln(x))\n",
        "\n",
        "    def __repr__(self):\n",
        "        repr_dropout = f\", dropout={self.dropout.p}\" if self.dropout else \"\"\n",
        "        return (\n",
        "            f\"NormedLinear(in_features={self.in_features}, \"\n",
        "            f\"out_features={self.out_features}, \"\n",
        "            f\"bias={self.bias is not None}{repr_dropout}, \"\n",
        "            f\"act={self.act.__class__.__name__})\"\n",
        "        )\n",
        "\n",
        "\n",
        "def mlp(in_dim, mlp_dims, out_dim, act=None, dropout=0.0):\n",
        "    \"\"\"\n",
        "    Basic building block of TD-MPC2.\n",
        "    MLP with LayerNorm, Mish activations, and optionally dropout.\n",
        "    \"\"\"\n",
        "    if isinstance(mlp_dims, int):\n",
        "        mlp_dims = [mlp_dims]\n",
        "    dims = [in_dim] + mlp_dims + [out_dim]\n",
        "    mlp = nn.ModuleList()\n",
        "    for i in range(len(dims) - 2):\n",
        "        mlp.append(NormedLinear(dims[i], dims[i + 1], dropout=dropout * (i == 0)))\n",
        "    mlp.append(\n",
        "        NormedLinear(dims[-2], dims[-1], act=act)\n",
        "        if act\n",
        "        else nn.Linear(dims[-2], dims[-1])\n",
        "    )\n",
        "    return nn.Sequential(*mlp)\n",
        "\n",
        "\n",
        "def conv(in_shape, num_channels, act=None):\n",
        "    \"\"\"\n",
        "    Basic convolutional encoder for TD-MPC2 with raw image observations.\n",
        "    4 layers of convolution with ReLU activations, followed by a linear layer.\n",
        "    \"\"\"\n",
        "    assert in_shape[-1] == 64  # assumes rgb observations to be 64x64\n",
        "    layers = [\n",
        "        ShiftAug(),\n",
        "        PixelPreprocess(),\n",
        "        nn.Conv2d(in_shape[0], num_channels, 7, stride=2),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(num_channels, num_channels, 5, stride=2),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(num_channels, num_channels, 3, stride=2),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(num_channels, num_channels, 3, stride=1),\n",
        "        nn.Flatten(),\n",
        "    ]\n",
        "    if act:\n",
        "        layers.append(act)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def enc(cfg, out={}):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of encoders for each observation in the dict.\n",
        "    \"\"\"\n",
        "    for k in cfg.obs_shape.keys():\n",
        "        if k == \"state\":\n",
        "            out[k] = mlp(\n",
        "                cfg.obs_shape[k][0] + cfg.task_dim,\n",
        "                max(cfg.num_enc_layers - 1, 1) * [cfg.enc_dim],\n",
        "                cfg.latent_dim,\n",
        "                act=SimNorm(cfg),\n",
        "            )\n",
        "        elif k == \"rgb\":\n",
        "            out[k] = conv(cfg.obs_shape[k], cfg.num_channels, act=SimNorm(cfg))\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f\"Encoder for observation type {k} not implemented.\"\n",
        "            )\n",
        "    return nn.ModuleDict(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4ef89a56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# math\n",
        "def soft_ce(pred, target, cfg):\n",
        "    \"\"\"Computes the cross entropy loss between predictions and soft targets.\"\"\"\n",
        "    pred = F.log_softmax(pred, dim=-1)\n",
        "    target = two_hot(target, cfg)\n",
        "    return -(target * pred).sum(-1, keepdim=True)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def _log_std(x, low, dif):\n",
        "    return low + 0.5 * dif * (torch.tanh(x) + 1)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def _gaussian_residual(eps, log_std):\n",
        "    return -0.5 * eps.pow(2) - log_std\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def _gaussian_logprob(residual):\n",
        "    return residual - 0.5 * torch.log(2 * torch.pi)\n",
        "\n",
        "\n",
        "def gaussian_logprob(eps, log_std, size=None):\n",
        "    \"\"\"Compute Gaussian log probability.\"\"\"\n",
        "    residual = _gaussian_residual(eps, log_std).sum(-1, keepdim=True)\n",
        "    if size is None:\n",
        "        size = eps.size(-1)\n",
        "    return _gaussian_logprob(residual) * size\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def _squash(pi):\n",
        "    return torch.log(F.relu(1 - pi.pow(2)) + 1e-6)\n",
        "\n",
        "\n",
        "def squash(mu, pi, log_pi):\n",
        "    \"\"\"Apply squashing function.\"\"\"\n",
        "    mu = torch.tanh(mu)\n",
        "    pi = torch.tanh(pi)\n",
        "    log_pi -= _squash(pi).sum(-1, keepdim=True)\n",
        "    return mu, pi, log_pi\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def symlog(x):\n",
        "    \"\"\"\n",
        "    Symmetric logarithmic function.\n",
        "    Adapted from https://github.com/danijar/dreamerv3.\n",
        "    \"\"\"\n",
        "    return torch.sign(x) * torch.log(1 + torch.abs(x))\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def symexp(x):\n",
        "    \"\"\"\n",
        "    Symmetric exponential function.\n",
        "    Adapted from https://github.com/danijar/dreamerv3.\n",
        "    \"\"\"\n",
        "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)\n",
        "\n",
        "\n",
        "def two_hot(x, cfg):\n",
        "    \"\"\"Converts a batch of scalars to soft two-hot encoded targets for discrete regression.\"\"\"\n",
        "    if cfg.num_bins == 0:\n",
        "        return x\n",
        "    elif cfg.num_bins == 1:\n",
        "        return symlog(x)\n",
        "    x = torch.clamp(symlog(x), cfg.vmin, cfg.vmax).squeeze(1)\n",
        "    bin_idx = torch.floor((x - cfg.vmin) / cfg.bin_size).long()\n",
        "    bin_offset = ((x - cfg.vmin) / cfg.bin_size - bin_idx.float()).unsqueeze(-1)\n",
        "    soft_two_hot = torch.zeros(x.size(0), cfg.num_bins, device=x.device)\n",
        "    soft_two_hot.scatter_(1, bin_idx.unsqueeze(1), 1 - bin_offset)\n",
        "    soft_two_hot.scatter_(1, (bin_idx.unsqueeze(1) + 1) % cfg.num_bins, bin_offset)\n",
        "    return soft_two_hot\n",
        "\n",
        "\n",
        "DREG_BINS = None\n",
        "\n",
        "\n",
        "def two_hot_inv(x, cfg):\n",
        "    \"\"\"Converts a batch of soft two-hot encoded vectors to scalars.\"\"\"\n",
        "    global DREG_BINS\n",
        "    if cfg.num_bins == 0:\n",
        "        return x\n",
        "    elif cfg.num_bins == 1:\n",
        "        return symexp(x)\n",
        "    if DREG_BINS is None:\n",
        "        DREG_BINS = torch.linspace(cfg.vmin, cfg.vmax, cfg.num_bins, device=x.device)\n",
        "    x = F.softmax(x, dim=-1)\n",
        "    x = torch.sum(x * DREG_BINS, dim=-1, keepdim=True)\n",
        "    return symexp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7da49852",
      "metadata": {},
      "outputs": [],
      "source": [
        "# world model\n",
        "class WorldModel(nn.Module):\n",
        "    \"\"\"\n",
        "    TD-MPC2 implicit world model architecture.\n",
        "    Can be used for both single-task and multi-task experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        if cfg.multitask:\n",
        "            self._task_emb = nn.Embedding(len(cfg.tasks), cfg.task_dim, max_norm=1)\n",
        "            self._action_masks = torch.zeros(len(cfg.tasks), cfg.action_dim)\n",
        "            for i in range(len(cfg.tasks)):\n",
        "                self._action_masks[i, : cfg.action_dims[i]] = 1.0\n",
        "        self._encoder = enc(cfg)\n",
        "        self._dynamics = mlp(\n",
        "            cfg.latent_dim + cfg.action_dim + cfg.task_dim,\n",
        "            2 * [cfg.mlp_dim],\n",
        "            cfg.latent_dim,\n",
        "            act=SimNorm(cfg),\n",
        "        )\n",
        "        self._reward = mlp(\n",
        "            cfg.latent_dim + cfg.action_dim + cfg.task_dim,\n",
        "            2 * [cfg.mlp_dim],\n",
        "            max(cfg.num_bins, 1),\n",
        "        )\n",
        "        self._pi = mlp(\n",
        "            cfg.latent_dim + cfg.task_dim, 2 * [cfg.mlp_dim], 2 * cfg.action_dim\n",
        "        )\n",
        "        self._Qs = Ensemble(\n",
        "            [\n",
        "                mlp(\n",
        "                    cfg.latent_dim + cfg.action_dim + cfg.task_dim,\n",
        "                    2 * [cfg.mlp_dim],\n",
        "                    max(cfg.num_bins, 1),\n",
        "                    dropout=cfg.dropout,\n",
        "                )\n",
        "                for _ in range(cfg.num_q)\n",
        "            ]\n",
        "        )\n",
        "        self.apply(weight_init)\n",
        "        zero_([self._reward[-1].weight, self._Qs.params[-2]])\n",
        "        self._target_Qs = deepcopy(self._Qs).requires_grad_(False)\n",
        "        self.log_std_min = torch.tensor(cfg.log_std_min)\n",
        "        self.log_std_dif = torch.tensor(cfg.log_std_max) - self.log_std_min\n",
        "\n",
        "    @property\n",
        "    def total_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def to(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Overriding `to` method to also move additional tensors to device.\n",
        "        \"\"\"\n",
        "        super().to(*args, **kwargs)\n",
        "        if self.cfg.multitask:\n",
        "            self._action_masks = self._action_masks.to(*args, **kwargs)\n",
        "        self.log_std_min = self.log_std_min.to(*args, **kwargs)\n",
        "        self.log_std_dif = self.log_std_dif.to(*args, **kwargs)\n",
        "        return self\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"\n",
        "        Overriding `train` method to keep target Q-networks in eval mode.\n",
        "        \"\"\"\n",
        "        super().train(mode)\n",
        "        self._target_Qs.train(False)\n",
        "        return self\n",
        "\n",
        "    def track_q_grad(self, mode=True):\n",
        "        \"\"\"\n",
        "        Enables/disables gradient tracking of Q-networks.\n",
        "        Avoids unnecessary computation during policy optimization.\n",
        "        This method also enables/disables gradients for task embeddings.\n",
        "        \"\"\"\n",
        "        for p in self._Qs.parameters():\n",
        "            p.requires_grad_(mode)\n",
        "        if self.cfg.multitask:\n",
        "            for p in self._task_emb.parameters():\n",
        "                p.requires_grad_(mode)\n",
        "\n",
        "    def soft_update_target_Q(self):\n",
        "        \"\"\"\n",
        "        Soft-update target Q-networks using Polyak averaging.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            for p, p_target in zip(self._Qs.parameters(), self._target_Qs.parameters()):\n",
        "                p_target.data.lerp_(p.data, self.cfg.tau)\n",
        "\n",
        "    def task_emb(self, x, task):\n",
        "        \"\"\"\n",
        "        Continuous task embedding for multi-task experiments.\n",
        "        Retrieves the task embedding for a given task ID `task`\n",
        "        and concatenates it to the input `x`.\n",
        "        \"\"\"\n",
        "        if isinstance(task, int):\n",
        "            task = torch.tensor([task], device=x.device)\n",
        "        emb = self._task_emb(task.long())\n",
        "        if x.ndim == 3:\n",
        "            emb = emb.unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
        "        elif emb.shape[0] == 1:\n",
        "            emb = emb.repeat(x.shape[0], 1)\n",
        "        return torch.cat([x, emb], dim=-1)\n",
        "\n",
        "    def encode(self, obs, task):\n",
        "        \"\"\"\n",
        "        Encodes an observation into its latent representation.\n",
        "        This implementation assumes a single state-based observation.\n",
        "        \"\"\"\n",
        "        if self.cfg.multitask:\n",
        "            obs = self.task_emb(obs, task)\n",
        "        if self.cfg.obs == \"rgb\" and obs.ndim == 5:\n",
        "            return torch.stack([self._encoder[self.cfg.obs](o) for o in obs])\n",
        "        return self._encoder[self.cfg.obs](obs)\n",
        "\n",
        "    def next(self, z, a, task):\n",
        "        \"\"\"\n",
        "        Predicts the next latent state given the current latent state and action.\n",
        "        \"\"\"\n",
        "        if self.cfg.multitask:\n",
        "            z = self.task_emb(z, task)\n",
        "        z = torch.cat([z, a], dim=-1)\n",
        "        return self._dynamics(z)\n",
        "\n",
        "    def reward(self, z, a, task):\n",
        "        \"\"\"\n",
        "        Predicts instantaneous (single-step) reward.\n",
        "        \"\"\"\n",
        "        if self.cfg.multitask:\n",
        "            z = self.task_emb(z, task)\n",
        "        z = torch.cat([z, a], dim=-1)\n",
        "        return self._reward(z)\n",
        "\n",
        "    def pi(self, z, task):\n",
        "        \"\"\"\n",
        "        Samples an action from the policy prior.\n",
        "        The policy prior is a Gaussian distribution with\n",
        "        mean and (log) std predicted by a neural network.\n",
        "        \"\"\"\n",
        "        if self.cfg.multitask:\n",
        "            z = self.task_emb(z, task)\n",
        "\n",
        "        # Gaussian policy prior\n",
        "        mu, log_std = self._pi(z).chunk(2, dim=-1)\n",
        "        log_std = _log_std(log_std, self.log_std_min, self.log_std_dif)\n",
        "        eps = torch.randn_like(mu)\n",
        "\n",
        "        if self.cfg.multitask:  # Mask out unused action dimensions\n",
        "            mu = mu * self._action_masks[task]\n",
        "            log_std = log_std * self._action_masks[task]\n",
        "            eps = eps * self._action_masks[task]\n",
        "            action_dims = self._action_masks.sum(-1)[task].unsqueeze(-1)\n",
        "        else:  # No masking\n",
        "            action_dims = None\n",
        "\n",
        "        log_pi = gaussian_logprob(eps, log_std, size=action_dims)\n",
        "        pi = mu + eps * log_std.exp()\n",
        "        mu, pi, log_pi = squash(mu, pi, log_pi)\n",
        "\n",
        "        return mu, pi, log_pi, log_std\n",
        "    \n",
        "    def log_pi_action(self, z, a, task):\n",
        "        \"\"\"\n",
        "        Compute the log probability of an action sequence given the latent states.\n",
        "        \"\"\"\n",
        "        if self.cfg.multitask:\n",
        "            z = self.task_emb(z, task)\n",
        "        mu, log_std = self._pi(z).chunk(2, dim=-1)\n",
        "        eps = (a - mu) / (log_std.exp() + 1e-8)\n",
        "\n",
        "        if self.cfg.multitask:  # Mask out unused action dimensions\n",
        "            mu = mu * self._action_masks[task]\n",
        "            log_std = log_std * self._action_masks[task]\n",
        "            eps = eps * self._action_masks[task]\n",
        "            action_dims = self._action_masks.sum(-1)[task].unsqueeze(-1)\n",
        "        else:  # No masking\n",
        "            action_dims = None\n",
        "            \n",
        "        log_pi = gaussian_logprob(eps, log_std, size=action_dims)\n",
        "        return log_pi\n",
        "\n",
        "    def Q(self, z, a, task, return_type=\"min\", target=False):\n",
        "        \"\"\"\n",
        "        Predict state-action value.\n",
        "        `return_type` can be one of [`min`, `avg`, `all`]:\n",
        "                - `min`: return the minimum of two randomly subsampled Q-values.\n",
        "                - `avg`: return the average of two randomly subsampled Q-values.\n",
        "                - `max`: return the maximum of two randomly subsampled Q-values.\n",
        "                - `all`: return all Q-values.\n",
        "        `target` specifies whether to use the target Q-networks or not.\n",
        "        \"\"\"\n",
        "        assert return_type in {\"min\", \"avg\", \"all\", \"max\"}\n",
        "\n",
        "        if self.cfg.multitask:\n",
        "            z = self.task_emb(z, task)\n",
        "        z = torch.cat([z, a], dim=-1)\n",
        "        out = (self._target_Qs if target else self._Qs)(z)\n",
        "\n",
        "        if return_type == \"all\":\n",
        "            return out\n",
        "\n",
        "        Q1, Q2 = out[np.random.choice(self.cfg.num_q, 2, replace=False)]\n",
        "        Q1, Q2 = two_hot_inv(Q1, self.cfg), two_hot_inv(Q2, self.cfg)\n",
        "\n",
        "        if return_type == \"min\":\n",
        "            return torch.min(Q1, Q2)\n",
        "        elif return_type == \"avg\":\n",
        "            return (Q1 + Q2) / 2\n",
        "        elif return_type == \"max\":\n",
        "            qs_thot = [two_hot_inv(q, self.cfg) for q in out]\n",
        "            qs_thot = torch.stack(qs_thot, dim=0)\n",
        "            return torch.max(qs_thot, dim=0)[0]\n",
        "        ##return torch.min(Q1, Q2) if return_type == \"min\" else (Q1 + Q2) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e995ad49",
      "metadata": {},
      "outputs": [],
      "source": [
        "# running scale\n",
        "class RunningScale:\n",
        "    \"\"\"Running trimmed scale estimator.\"\"\"\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "        self._value = torch.ones(1, dtype=torch.float32, device=self.device)\n",
        "        self._percentiles = torch.tensor(\n",
        "            [5, 95], dtype=torch.float32, device=self.device\n",
        "        )\n",
        "\n",
        "    def state_dict(self):\n",
        "        return dict(value=self._value, percentiles=self._percentiles)\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self._value.data.copy_(state_dict[\"value\"])\n",
        "        self._percentiles.data.copy_(state_dict[\"percentiles\"])\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self._value.cpu().item()\n",
        "\n",
        "    def _percentile(self, x):\n",
        "        x_dtype, x_shape = x.dtype, x.shape\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        in_sorted, _ = torch.sort(x, dim=0)\n",
        "        positions = self._percentiles * (x.shape[0] - 1) / 100\n",
        "        floored = torch.floor(positions)\n",
        "        ceiled = floored + 1\n",
        "        ceiled[ceiled > x.shape[0] - 1] = x.shape[0] - 1\n",
        "        weight_ceiled = positions - floored\n",
        "        weight_floored = 1.0 - weight_ceiled\n",
        "        d0 = in_sorted[floored.long(), :] * weight_floored[:, None]\n",
        "        d1 = in_sorted[ceiled.long(), :] * weight_ceiled[:, None]\n",
        "        return (d0 + d1).view(-1, *x_shape[1:]).type(x_dtype)\n",
        "\n",
        "    def update(self, x):\n",
        "        percentiles = self._percentile(x.detach())\n",
        "        value = torch.clamp(percentiles[1] - percentiles[0], min=1.0)\n",
        "        self._value.data.lerp_(value, self.cfg.tau)\n",
        "\n",
        "    def __call__(self, x, update=False):\n",
        "        if update:\n",
        "            self.update(x)\n",
        "        return x * (1 / self.value)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"RunningScale(S: {self.value})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3e9fa642",
      "metadata": {},
      "outputs": [],
      "source": [
        "# td-mpc2\n",
        "class TDMPC2:\n",
        "\t\"\"\"\n",
        "\tModified TD-MPC2 agent. Implements training + inference.\n",
        "\tCurrent implementation supports both state and pixel observations.\n",
        "\tOnly support Single-task setting is supported.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, cfg):\n",
        "\t\tself.cfg = cfg\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\tself.device = torch.device(\"cuda\")\n",
        "\t\telse:\n",
        "\t\t\tself.device = torch.device(\"cpu\")\n",
        "\t\tself.model = WorldModel(cfg).to(self.device)\n",
        "\t\tself.optim = torch.optim.Adam(\n",
        "\t\t\t[\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t\"params\": self.model._encoder.parameters(),\n",
        "\t\t\t\t\t\"lr\": self.cfg.lr * self.cfg.enc_lr_scale,\n",
        "\t\t\t\t},\n",
        "\t\t\t\t{\"params\": self.model._dynamics.parameters()},\n",
        "\t\t\t\t{\"params\": self.model._reward.parameters()},\n",
        "\t\t\t\t{\"params\": self.model._Qs.parameters()},\n",
        "\t\t\t\t{\n",
        "\t\t\t\t\t\"params\": self.model._task_emb.parameters()\n",
        "\t\t\t\t\tif self.cfg.multitask\n",
        "\t\t\t\t\telse []\n",
        "\t\t\t\t},\n",
        "\t\t\t],\n",
        "\t\t\tlr=self.cfg.lr,\n",
        "\t\t)\n",
        "\t\tself.pi_optim = torch.optim.Adam(\n",
        "\t\t\tself.model._pi.parameters(), lr=self.cfg.lr, eps=1e-5\n",
        "\t\t)\n",
        "\t\tself.model.eval()\n",
        "\t\tself.scale = RunningScale(cfg)\n",
        "\t\tself.log_pi_scale = RunningScale(cfg) # policy log-probability scale\n",
        "\t\tself.cfg.iterations += 2 * int(\n",
        "\t\t\tcfg.action_dim >= 20\n",
        "\t\t)  # Heuristic for large action spaces\n",
        "\t\tself.discount = (\n",
        "\t\t\ttorch.tensor(\n",
        "\t\t\t\t[self._get_discount(ep_len) for ep_len in cfg.episode_lengths],\n",
        "\t\t\t\tdevice=\"cuda\",\n",
        "\t\t\t)\n",
        "\t\t\tif self.cfg.multitask\n",
        "\t\t\telse self._get_discount(cfg.episode_length)\n",
        "\t\t)\n",
        "\n",
        "\tdef _get_discount(self, episode_length):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns discount factor for a given episode length.\n",
        "\t\tSimple heuristic that scales discount linearly with episode length.\n",
        "\t\tDefault values should work well for most tasks, but can be changed as needed.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tepisode_length (int): Length of the episode. Assumes episodes are of fixed length.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\tfloat: Discount factor for the task.\n",
        "\t\t\"\"\"\n",
        "\t\tfrac = episode_length / self.cfg.discount_denom\n",
        "\t\treturn min(\n",
        "\t\t\tmax((frac - 1) / (frac), self.cfg.discount_min), self.cfg.discount_max\n",
        "\t\t)\n",
        "\n",
        "\tdef save(self, fp):\n",
        "\t\t\"\"\"\n",
        "\t\tSave state dict of the agent to filepath.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tfp (str): Filepath to save state dict to.\n",
        "\t\t\"\"\"\n",
        "\t\ttorch.save({\"model\": self.model.state_dict()}, fp)\n",
        "\n",
        "\tdef load(self, fp):\n",
        "\t\t\"\"\"\n",
        "\t\tLoad a saved state dict from filepath (or dictionary) into current agent.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tfp (str or dict): Filepath or state dict to load.\n",
        "\t\t\"\"\"\n",
        "\t\tstate_dict = fp if isinstance(fp, dict) else torch.load(fp)\n",
        "\t\tself.model.load_state_dict(state_dict[\"model\"])\n",
        "\n",
        "\t@torch.no_grad()\n",
        "\tdef act(self, obs, t0=False, eval_mode=False, task=None, use_pi=False):\n",
        "\t\t\"\"\"\n",
        "\t\tSelect an action by planning in the latent space of the world model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tobs (torch.Tensor): Observation from the environment.\n",
        "\t\t\t\tt0 (bool): Whether this is the first observation in the episode.\n",
        "\t\t\t\teval_mode (bool): Whether to use the mean of the action distribution.\n",
        "\t\t\t\ttask (int): Task index (only used for multi-task experiments).\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\ttorch.Tensor: Action to take in the environment.\n",
        "\t\t\"\"\"\n",
        "\t\t# obsをtorch.Tensorに変換\n",
        "\t\tobs = torch.tensor(obs, device=self.device).unsqueeze(0)\n",
        "\t\t# obs = obs.to(self.device, non_blocking=True).unsqueeze(0)\n",
        "\t\tif task is not None: # 絶対に実行されない\n",
        "\t\t\ttask = torch.tensor([task], device=self.device)\n",
        "\t\tz = self.model.encode(obs, task) # 画像を潜在空間にエンコード\n",
        "\t\tif self.cfg.mpc and not use_pi:\n",
        "\t\t\ta, mu, std = self.plan(z, t0=t0, eval_mode=eval_mode, task=task)\n",
        "\t\telse: # こちらは実行されない\n",
        "\t\t\tmu, pi, log_pi, log_std = self.model.pi(z, task)\n",
        "\t\t\tif eval_mode:\n",
        "\t\t\t\ta = mu[0]\n",
        "\t\t\telse:\n",
        "\t\t\t\ta = pi[0]\n",
        "\t\t\tmu, std = mu[0], log_std.exp()[0]\n",
        "\t\tif len(a.shape) == 1:\n",
        "\t\t\ta = a.unsqueeze(0)\n",
        "\t\treturn a.cpu(), mu.cpu(), std.cpu()\n",
        "\n",
        "\t@torch.no_grad()\n",
        "\tdef _estimate_value(self, z, actions, task, horizon, eval_mode=False):\n",
        "\t\t\"\"\"Estimate value of a trajectory starting at latent state z and executing given actions.\"\"\"\n",
        "\t\tG, discount = 0, 1\n",
        "\t\tfor t in range(horizon):\n",
        "\t\t\treward = two_hot_inv(self.model.reward(z, actions[t], task), self.cfg)\n",
        "\t\t\tz = self.model.next(z, actions[t], task)\n",
        "\t\t\tG += discount * reward\n",
        "\t\t\tdiscount *= (\n",
        "\t\t\t\tself.discount[torch.tensor(task)]\n",
        "\t\t\t\tif self.cfg.multitask\n",
        "\t\t\t\telse self.discount\n",
        "\t\t\t)\n",
        "\t\treturn G + discount * self.model.Q(\n",
        "\t\t\tz, self.model.pi(z, task)[1], task, return_type=\"avg\"\n",
        "\t\t)\n",
        "\n",
        "\t@torch.no_grad()\n",
        "\tdef _estimate_value_parallel(self, z, actions, task):\n",
        "\t\t\"\"\"Estimate value of a trajectory starting at latent state z and executing given actions.\"\"\"\n",
        "\t\tG, discount = 0, 1\n",
        "\t\tfor t in range(self.cfg.horizon):\n",
        "\t\t\treward = two_hot_inv(self.model.reward(z, actions[:, t], task), self.cfg)\n",
        "\t\t\tz = self.model.next(z, actions[:, t], task)\n",
        "\t\t\tG = G + discount * reward\n",
        "\t\t\tdiscount_update = self.discount[torch.tensor(task)] if self.cfg.multitask else self.discount\n",
        "\t\t\tdiscount = discount * discount_update\n",
        "\t\treturn G + discount * self.model.Q(z, self.model.pi(z, task)[1], task, return_type='avg')\n",
        "\n",
        "\t@torch.no_grad()\n",
        "\tdef plan(self, z, t0=False, eval_mode=False, task=None):\n",
        "\t\t\"\"\"\n",
        "\t\tPlan a sequence of actions using the learned world model.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tz (torch.Tensor): Latent state from which to plan.\n",
        "\t\t\t\tt0 (bool): Whether this is the first observation in the episode.\n",
        "\t\t\t\teval_mode (bool): Whether to use the mean of the action distribution.\n",
        "\t\t\t\ttask (Torch.Tensor): Task index (only used for multi-task experiments).\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\ttorch.Tensor: Action to take in the environment.\n",
        "\t\t\"\"\"\n",
        "\t\tif self.cfg.num_pi_trajs > 0:\n",
        "\t\t\tpi_actions = torch.empty(\n",
        "\t\t\t\tself.cfg.horizon,\n",
        "\t\t\t\tself.cfg.num_pi_trajs,\n",
        "\t\t\t\tself.cfg.action_dim,\n",
        "\t\t\t\tdevice=self.device,\n",
        "\t\t\t)\n",
        "\t\t\t_z = z.repeat(self.cfg.num_pi_trajs, 1)\n",
        "\t\t\tfor t in range(self.cfg.horizon - 1):\n",
        "\t\t\t\tpi_actions[t] = self.model.pi(_z, task)[1]\n",
        "\t\t\t\t_z = self.model.next(_z, pi_actions[t], task)\n",
        "\t\t\tpi_actions[-1] = self.model.pi(_z, task)[1]\n",
        "\n",
        "\t\t# Initialize state and parameters\n",
        "\t\tz = z.repeat(self.cfg.num_samples, 1)\n",
        "\t\tmean = torch.zeros(self.cfg.horizon, self.cfg.action_dim, device=self.device)\n",
        "\t\tstd = self.cfg.max_std * torch.ones(\n",
        "\t\t\tself.cfg.horizon, self.cfg.action_dim, device=self.device\n",
        "\t\t)\n",
        "\t\tif not t0:\n",
        "\t\t\tmean[:-1] = self._prev_mean[1:]\n",
        "\t\tactions = torch.empty(\n",
        "\t\t\tself.cfg.horizon,\n",
        "\t\t\tself.cfg.num_samples,\n",
        "\t\t\tself.cfg.action_dim,\n",
        "\t\t\tdevice=self.device,\n",
        "\t\t)\n",
        "\t\tif self.cfg.num_pi_trajs > 0:\n",
        "\t\t\tactions[:, : self.cfg.num_pi_trajs] = pi_actions\n",
        "\n",
        "\t\t# Iterate MPPI\n",
        "\t\tfor _ in range(self.cfg.iterations):\n",
        "\t\t\t# Sample actions\n",
        "\t\t\tactions[:, self.cfg.num_pi_trajs :] = (\n",
        "\t\t\t\tmean.unsqueeze(1)\n",
        "\t\t\t\t+ std.unsqueeze(1)\n",
        "\t\t\t\t* torch.randn(\n",
        "\t\t\t\t\tself.cfg.horizon,\n",
        "\t\t\t\t\tself.cfg.num_samples - self.cfg.num_pi_trajs,\n",
        "\t\t\t\t\tself.cfg.action_dim,\n",
        "\t\t\t\t\tdevice=std.device,\n",
        "\t\t\t\t)\n",
        "\t\t\t).clamp(-1, 1)\n",
        "\t\t\tif self.cfg.multitask:\n",
        "\t\t\t\tactions = actions * self.model._action_masks[task]\n",
        "\n",
        "\t\t\t# Compute elite actions\n",
        "\t\t\tvalue = self._estimate_value(z, actions, task, self.cfg.horizon).nan_to_num_(0)\n",
        "\t\t\telite_idxs = torch.topk(\n",
        "\t\t\t\tvalue.squeeze(1), self.cfg.num_elites, dim=0\n",
        "\t\t\t).indices\n",
        "\t\t\telite_value, elite_actions = value[elite_idxs], actions[:, elite_idxs]\n",
        "\n",
        "\t\t\t# Update parameters\n",
        "\t\t\tmax_value = elite_value.max(0)[0]\n",
        "\t\t\tscore = torch.exp(self.cfg.temperature * (elite_value - max_value))\n",
        "\t\t\tscore /= score.sum(0)\n",
        "\t\t\tmean = torch.sum(score.unsqueeze(0) * elite_actions, dim=1) / (\n",
        "\t\t\t\tscore.sum(0) + 1e-9\n",
        "\t\t\t)\n",
        "\t\t\tstd = torch.sqrt(\n",
        "\t\t\t\ttorch.sum(\n",
        "\t\t\t\t\tscore.unsqueeze(0) * (elite_actions - mean.unsqueeze(1)) ** 2, dim=1\n",
        "\t\t\t\t)\n",
        "\t\t\t\t/ (score.sum(0) + 1e-9)\n",
        "\t\t\t).clamp_(self.cfg.min_std, self.cfg.max_std)\n",
        "\t\t\tif self.cfg.multitask:\n",
        "\t\t\t\tmean = mean * self.model._action_masks[task]\n",
        "\t\t\t\tstd = std * self.model._action_masks[task]\n",
        "\n",
        "\t\t# Select action\n",
        "\t\tscore = score.squeeze(1).cpu().numpy()\n",
        "\t\tactions = elite_actions[:, np.random.choice(np.arange(score.shape[0]), p=score)]\n",
        "\t\tself._prev_mean = mean\n",
        "\t\tmu, std = actions[0], std[0]\n",
        "\t\tif not eval_mode:\n",
        "\t\t\ta = mu + std * torch.randn(self.cfg.action_dim, device=std.device)\n",
        "\t\telse:\n",
        "\t\t\ta = mu\n",
        "\t\treturn a.clamp_(-1, 1), mu, std\n",
        "\n",
        "\tdef update_pi(self, zs, action, mu, std, task):\n",
        "\t\t\"\"\"\n",
        "\t\tUpdate policy using a sequence of latent states.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tzs (torch.Tensor): Sequence of latent states.\n",
        "\t\t\t\taction (torch.Tensor): Sequence of actions.\n",
        "\t\t\t\ttask (torch.Tensor): Task index (only used for multi-task experiments).\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\tfloat: Loss of the policy update.\n",
        "\t\t\"\"\"\n",
        "\t\tself.pi_optim.zero_grad(set_to_none=True)\n",
        "\t\tself.model.track_q_grad(False)\n",
        "\t\t_, pis, log_pis, _ = self.model.pi(zs, task)\n",
        "\t\tqs = self.model.Q(zs, pis, task, return_type=\"avg\")\n",
        "\t\tself.scale.update(qs[0])\n",
        "\t\tqs = self.scale(qs)\n",
        "\t\t\t\n",
        "\t\trho = torch.pow(self.cfg.rho, torch.arange(len(qs), device=self.device))\n",
        "\t\tif self.cfg.actor_mode==\"sac\":\n",
        "\t\t\t# TD-MPC2 baseline setting.\n",
        "\t\t\tpi_loss = ((self.cfg.entropy_coef * log_pis - qs).mean(dim=(1, 2)) * rho).mean()\n",
        "\t\t\tprior_loss = torch.zeros_like(pi_loss) # Not used\n",
        "\t\t\tq_loss = pi_loss.detach().clone()\n",
        "\n",
        "\t\telif self.cfg.actor_mode==\"awac\":\n",
        "\t\t\t# Loss for AWAC-MPC\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tvs = self.model.Q(zs, action, task, return_type=\"avg\")\n",
        "\t\t\t\tvs = self.scale(vs)\n",
        "\t\t\tadv = (qs - vs).detach()\n",
        "\t\t\tweights = torch.clamp(torch.exp(adv / self.cfg.awac_lambda), self.cfg.exp_adv_min, self.cfg.exp_adv_max)\n",
        "\t\t\tlog_pis_action = self.model.log_pi_action(zs, action, task)\n",
        "\t\t\tpi_loss = (( - weights * log_pis_action).mean(dim=(1, 2)) * rho).mean()\n",
        "\t\t\tq_loss = torch.zeros_like(pi_loss)\n",
        "\t\t\tprior_loss = torch.zeros_like(pi_loss)\n",
        "\n",
        "\t\telif self.cfg.actor_mode==\"residual\":\n",
        "\t\t\t# Loss for TD-M(PC)^2\n",
        "\t\t\taction_dims = None if not self.cfg.multitask else self.model._action_masks.size(-1)\n",
        "\t\t\tstd = torch.max(std, self.cfg.min_std * torch.ones_like(std))\n",
        "\t\t\teps = (pis - mu) / std\n",
        "\t\t\tlog_pis_prior = gaussian_logprob(eps, std.log(), size=action_dims).mean(dim=-1)\n",
        "\t\t\t#log_pis_prior = torch.clamp(log_pis_prior, -50000, 0.0)\n",
        "\n",
        "\t\t\tlog_pis_prior = self.scale(log_pis_prior) if self.scale.value > self.cfg.scale_threshold else torch.zeros_like(log_pis_prior)\n",
        "\n",
        "\t\t\tq_loss = ((self.cfg.entropy_coef * log_pis - qs).mean(dim=(1, 2)) * rho).mean()\n",
        "\t\t\tprior_loss = - (log_pis_prior.mean(dim=-1) * rho).mean()\n",
        "\t\t\tpi_loss = q_loss + (self.cfg.prior_coef * self.cfg.action_dim / 61) * prior_loss\n",
        "\n",
        "\t\telif self.cfg.actor_mode==\"bc_sac\": \n",
        "\t\t\t# Vanilla BC-SAC loss for policy learning\n",
        "\t\t\tq_loss = ((self.cfg.entropy_coef * log_pis - qs).mean(dim=(1, 2)) * rho).mean()\n",
        "\t\t\tprior_loss = (((pis - action) ** 2).sum(dim=-1).mean(dim=1) * rho).mean()\n",
        "\t\t\tpi_loss = q_loss + self.cfg.prior_coef * prior_loss\n",
        "\n",
        "\t\telif self.cfg.actor_mode==\"bc\":\n",
        "\t\t\t# Loss for BC-MPC baseline\n",
        "\t\t\taction_dims = None if not self.cfg.multitask else self.model._action_masks.size(-1)\n",
        "\t\t\tstd = torch.max(std, self.cfg.min_std * torch.ones_like(std))\n",
        "\t\t\teps = (pis - mu) / std\n",
        "\t\t\tlog_pis_prior = gaussian_logprob(eps, std.log(), size=action_dims).mean(dim=-1)\n",
        "\t\t\tlog_pis_prior = torch.clamp(log_pis_prior, -50000, 0.0)\n",
        "\t\t\tself.log_pi_scale.update(log_pis_prior[0])\n",
        "\t\t\t\n",
        "\t\t\tlog_pis_prior = self.scale(log_pis_prior)\n",
        "\t\t\tpi_loss = - (log_pis_prior.mean(dim=-1) * rho).mean()\n",
        "\t\t\tprior_loss = pi_loss.detach().clone()\n",
        "\t\t\tq_loss = torch.zeros_like(pi_loss) # Not used\n",
        "\n",
        "\t\telse:\n",
        "\t\t\traise NotImplementedError\n",
        "\n",
        "\t\tpi_loss.backward()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(\n",
        "\t\t\tself.model._pi.parameters(), self.cfg.grad_clip_norm\n",
        "\t\t)\n",
        "\t\t\n",
        "\t\tself.pi_optim.step()\n",
        "\t\tself.model.track_q_grad(True)\n",
        "\n",
        "\t\treturn pi_loss.item(), q_loss.item(), prior_loss.item()\n",
        "\n",
        "\t@torch.no_grad()\n",
        "\tdef _td_target(self, next_z, reward, task):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute the TD-target from a reward and the observation at the following time step.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tnext_z (torch.Tensor): Latent state at the following time step.\n",
        "\t\t\t\treward (torch.Tensor): Reward at the current time step.\n",
        "\t\t\t\ttask (torch.Tensor): Task index (only used for multi-task experiments).\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\ttorch.Tensor: TD-target.\n",
        "\t\t\"\"\"\n",
        "\t\tpi = self.model.pi(next_z, task)[1]\n",
        "\t\tdiscount = (\n",
        "\t\t\tself.discount[task].unsqueeze(-1) if self.cfg.multitask else self.discount\n",
        "\t\t)\n",
        "\t\treturn reward + discount * self.model.Q(\n",
        "\t\t\tnext_z, pi, task, return_type=\"min\", target=True\n",
        "\t\t)\n",
        "\n",
        "\tdef update(self, buffer):\t\n",
        "\t\t\"\"\"\n",
        "\t\tMain update function. Corresponds to one iteration of model learning.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\t\tbuffer (common.buffer.Buffer): Replay buffer.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\t\tdict: Dictionary of training statistics.\n",
        "\t\t\"\"\"\n",
        "\t\tif self.cfg.multitask and self.cfg.task in {\"mt30\",\"mt80\"}:\n",
        "\t\t\t# offline training\n",
        "\t\t\tobs, action, reward, task = buffer.sample()\n",
        "\t\t\tmu = action.detach().clone()\n",
        "\t\t\tstd = torch.full_like(action, self.cfg.max_std)\n",
        "\t\telse:\n",
        "\t\t\t# online training\n",
        "\t\t\tobs, action, mu, std, reward, task = buffer.sample() # mu and std are from Gaussian policy used for data collection\t\n",
        "\n",
        "\t\t# Compute targets\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tnext_z = self.model.encode(obs[1:], task)\n",
        "\t\t\ttd_targets = self._td_target(next_z, reward, task)\n",
        "\t\t\t\n",
        "\t\t# Prepare for update\n",
        "\t\tself.optim.zero_grad(set_to_none=True)\n",
        "\t\tself.model.train()\n",
        "\n",
        "\t\t# Latent rollout\n",
        "\t\tzs = torch.empty(\n",
        "\t\t\tself.cfg.horizon + 1,\n",
        "\t\t\tself.cfg.batch_size,\n",
        "\t\t\tself.cfg.latent_dim,\n",
        "\t\t\tdevice=self.device,\n",
        "\t\t)\n",
        "\t\tz = self.model.encode(obs[0], task)\n",
        "\t\tzs[0] = z\n",
        "\t\tconsistency_loss = 0\n",
        "\t\tfor t in range(self.cfg.horizon):\n",
        "\t\t\tz = self.model.next(z, action[t], task)\n",
        "\t\t\tconsistency_loss += F.mse_loss(z, next_z[t]) * self.cfg.rho**t\n",
        "\t\t\tzs[t + 1] = z\n",
        "\n",
        "\t\t# Predictions\n",
        "\t\t_zs = zs[:-1]\n",
        "\t\tqs = self.model.Q(_zs, action, task, return_type=\"all\")\n",
        "\t\treward_preds = self.model.reward(_zs, action, task)\n",
        "\n",
        "\t\t# Compute losses\n",
        "\t\treward_loss, value_loss = 0, 0\n",
        "\t\tfor t in range(self.cfg.horizon):\n",
        "\t\t\treward_loss += (\n",
        "\t\t\t\tsoft_ce(reward_preds[t], reward[t], self.cfg).mean()\n",
        "\t\t\t\t* self.cfg.rho**t\n",
        "\t\t\t)\n",
        "\t\t\tfor q in range(self.cfg.num_q):\n",
        "\t\t\t\tvalue_loss += (\n",
        "\t\t\t\t\tsoft_ce(qs[q][t], td_targets[t], self.cfg).mean()\n",
        "\t\t\t\t\t* self.cfg.rho**t\n",
        "\t\t\t\t)\n",
        "\t\tconsistency_loss *= 1 / self.cfg.horizon\n",
        "\t\treward_loss *= 1 / self.cfg.horizon\n",
        "\t\tvalue_loss *= 1 / (self.cfg.horizon * self.cfg.num_q)\n",
        "\n",
        "\t\ttotal_loss = (\n",
        "\t\t\tself.cfg.consistency_coef * consistency_loss\n",
        "\t\t\t+ self.cfg.reward_coef * reward_loss\n",
        "\t\t\t+ self.cfg.value_coef * value_loss\n",
        "\t\t)\n",
        "\t\t# Update model\n",
        "\t\ttotal_loss.backward()\n",
        "\t\tgrad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "\t\t\tself.model.parameters(), self.cfg.grad_clip_norm\n",
        "\t\t)\n",
        "\t\tself.optim.step()\n",
        "\n",
        "\t\t# Update policy\n",
        "\t\tpi_loss, pi_loss_q, pi_loss_prior  = self.update_pi(_zs.detach(), action.detach(), mu.detach(), std.detach(), task)\n",
        "\n",
        "\t\t# Update target Q-functions\n",
        "\t\tself.model.soft_update_target_Q()\n",
        "\n",
        "\t\t# Return training statistics\n",
        "\t\tself.model.eval()\n",
        "\t\treturn {\n",
        "\t\t\t\"consistency_loss\": float(consistency_loss.mean().item()),\n",
        "\t\t\t\"reward_loss\": float(reward_loss.mean().item()),\n",
        "\t\t\t\"value_loss\": float(value_loss.mean().item()),\n",
        "\t\t\t\"pi_loss\": pi_loss,\n",
        "\t\t\t\"pi_loss_q\": pi_loss_q,\n",
        "\t\t\t\"pi_loss_prior\": pi_loss_prior,\n",
        "\t\t\t\"total_loss\": float(total_loss.mean().item()),\n",
        "\t\t\t\"grad_norm\": float(grad_norm),\n",
        "\t\t\t\"pi_scale\": float(self.scale.value)\n",
        "\t\t}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06c188f-8a87-42e7-9f61-7f385eccc565",
      "metadata": {
        "id": "b06c188f-8a87-42e7-9f61-7f385eccc565"
      },
      "source": [
        "## 5. 学習"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2f60ee0",
      "metadata": {},
      "source": [
        "Num|Action\n",
        "---|---\n",
        "0|NOOP\n",
        "1|UP\n",
        "2|RIGHT\n",
        "3|LEFT\n",
        "4|DOWN\n",
        "5|UPRIGHT\n",
        "6|UPLEFT\n",
        "7|DOWNRIGHT\n",
        "8|DOWNLEFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9b7690f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# logger\n",
        "\n",
        "CONSOLE_FORMAT = [\n",
        "    (\"iteration\", \"I\", \"int\"),\n",
        "    (\"episode\", \"E\", \"int\"),\n",
        "    (\"step\", \"I\", \"int\"),\n",
        "    (\"episode_reward\", \"R\", \"float\"),\n",
        "    (\"episode_success\", \"S\", \"float\"),\n",
        "    (\"total_time\", \"T\", \"time\"),\n",
        "]\n",
        "\n",
        "def make_dir(dir_path):\n",
        "    \"\"\"Create directory if it does not already exist.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(dir_path)\n",
        "    except OSError:\n",
        "        pass\n",
        "    return dir_path\n",
        "\n",
        "def print_run(cfg):\n",
        "    \"\"\"\n",
        "    Pretty-printing of current run information.\n",
        "    Logger calls this method at initialization.\n",
        "    \"\"\"\n",
        "    prefix, color, attrs = \"  \", \"green\", [\"bold\"]\n",
        "\n",
        "    def _limstr(s, maxlen=36):\n",
        "        return str(s[:maxlen]) + \"...\" if len(str(s)) > maxlen else s\n",
        "\n",
        "    def _pprint(k, v):\n",
        "        print(prefix + f'{k.capitalize()+\":\":<15}')\n",
        "\n",
        "    observations = \", \".join([str(v) for v in cfg.obs_shape.values()])\n",
        "    kvs = [\n",
        "        (\"task\", cfg.task),\n",
        "        (\"steps\", f\"{int(cfg.steps):,}\"),\n",
        "        (\"observations\", observations),\n",
        "        (\"actions\", cfg.action_dim),\n",
        "        (\"experiment\", cfg.exp_name),\n",
        "    ]\n",
        "    w = np.max([len(_limstr(str(kv[1]))) for kv in kvs]) + 25\n",
        "    div = \"-\" * w\n",
        "    print(div)\n",
        "    for k, v in kvs:\n",
        "        _pprint(k, v)\n",
        "    print(div)\n",
        "\n",
        "def cfg_to_group(cfg, return_list=False):\n",
        "    \"\"\"\n",
        "    Return a wandb-safe group name for logging.\n",
        "    Optionally returns group name as list.\n",
        "    \"\"\"\n",
        "    lst = [cfg.task, re.sub(\"[^0-9a-zA-Z]+\", \"-\", cfg.exp_name)]\n",
        "    return lst if return_list else \"-\".join(lst)\n",
        "\n",
        "class VideoRecorder:\n",
        "    \"\"\"Utility class for logging evaluation videos.\"\"\"\n",
        "\n",
        "    def __init__(self, cfg, wandb, fps=15):\n",
        "        self.cfg = cfg\n",
        "        self._save_dir = make_dir(cfg.work_dir / \"eval_video\")\n",
        "        self._wandb = wandb\n",
        "        self.fps = fps\n",
        "        self.frames = []\n",
        "        self.enabled = False\n",
        "\n",
        "    def init(self, env, enabled=True):\n",
        "        self.frames = []\n",
        "        self.enabled = self._save_dir and self._wandb and enabled\n",
        "        self.record(env)\n",
        "\n",
        "    def record(self, env):\n",
        "        if self.enabled:\n",
        "            self.frames.append(env.render(\"rgb_array\"))\n",
        "\n",
        "    def save(self, step, key=\"videos/eval_video\"):\n",
        "        if self.enabled and len(self.frames) > 0:\n",
        "            frames = np.stack(self.frames)\n",
        "            return self._wandb.log(\n",
        "                {\n",
        "                    key: self._wandb.Video(\n",
        "                        frames.transpose(0, 3, 1, 2), fps=self.fps, format=\"mp4\"\n",
        "                    )\n",
        "                },\n",
        "                step=step,\n",
        "            )\n",
        "\n",
        "class Logger:\n",
        "    \"\"\"Primary logging object. Logs either locally or using wandb.\"\"\"\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        self._log_dir = make_dir(cfg.work_dir)\n",
        "        self._model_dir = make_dir(self._log_dir / \"models\")\n",
        "        self._save_csv = cfg.save_csv\n",
        "        self._save_agent = cfg.save_agent\n",
        "        self._group = cfg_to_group(cfg)\n",
        "        self._seed = cfg.seed\n",
        "        self._eval = []\n",
        "        print_run(cfg)\n",
        "        self.project = cfg.wandb_project if cfg.wandb_project is not None else \"none\"\n",
        "        self.entity = cfg.wandb_entity if cfg.wandb_entity is not None else \"none\"\n",
        "        if cfg.disable_wandb or self.project == \"none\" or self.entity == \"none\":\n",
        "            print(\"Wandb disabled.\")\n",
        "            cfg.save_agent = False\n",
        "            cfg.save_video = False\n",
        "            self._wandb = None\n",
        "            self._video = None\n",
        "            return\n",
        "        os.environ[\"WANDB_SILENT\"] = \"true\" if cfg.wandb_silent else \"false\"\n",
        "\n",
        "        wandb.init(\n",
        "            project=self.project,\n",
        "            # entity=self.entity,\n",
        "            name=f\"{cfg.task}.tdmpc.{cfg.exp_name}.{cfg.seed}\",\n",
        "            #group=self._group,\n",
        "            # tags=cfg_to_group(cfg, return_list=True) + [f\"seed:{cfg.seed}\"],\n",
        "            dir=self._log_dir,\n",
        "            config=cfg.__dict__,\n",
        "        )\n",
        "        print(\"Logs will be synced with wandb.\")\n",
        "        self._wandb = wandb\n",
        "        self._video = (\n",
        "            VideoRecorder(cfg, self._wandb) if self._wandb and cfg.save_video else None\n",
        "        )\n",
        "\n",
        "\n",
        "    @property\n",
        "    def video(self):\n",
        "        return self._video\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        return self._model_dir\n",
        "\n",
        "    def save_agent(self, agent=None, identifier=\"final\"):\n",
        "        if self._save_agent and agent:\n",
        "            fp = self._model_dir / f\"{str(identifier)}.pt\"\n",
        "            agent.save(fp)\n",
        "            if self._wandb:\n",
        "                artifact = self._wandb.Artifact(\n",
        "                    self._group + \"-\" + str(self._seed) + \"-\" + str(identifier),\n",
        "                    type=\"model\",\n",
        "                )\n",
        "                artifact.add_file(fp)\n",
        "                self._wandb.log_artifact(artifact)\n",
        "\n",
        "    def finish(self, agent=None):\n",
        "        try:\n",
        "            self.save_agent(agent)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save model: {e}\")\n",
        "        if self._wandb:\n",
        "            self._wandb.finish()\n",
        "\n",
        "    def _format(self, key, value, ty):\n",
        "        if ty == \"int\":\n",
        "            return f'{key+\":\"} {int(value):,}'\n",
        "        elif ty == \"float\":\n",
        "            return f'{key+\":\"} {value:.01f}'\n",
        "        elif ty == \"time\":\n",
        "            value = str(datetime.timedelta(seconds=int(value)))\n",
        "            return f'{key+\":\"} {value}'\n",
        "        else:\n",
        "            raise f\"invalid log format type: {ty}\"\n",
        "\n",
        "    def _print(self, d, category):\n",
        "        pieces = [f\" {category:<14}\"]\n",
        "        for k, disp_k, ty in CONSOLE_FORMAT:\n",
        "            if k in d:\n",
        "                pieces.append(f\"{self._format(disp_k, d[k], ty):<22}\")\n",
        "        print(\"   \".join(pieces))\n",
        "\n",
        "    def log(self, d, category=\"train\"):\n",
        "        if self._wandb:\n",
        "            if category in {\"train\", \"eval\", \"results\"}:\n",
        "                xkey = \"step\"\n",
        "            elif category == \"pretrain\":\n",
        "                xkey = \"iteration\"\n",
        "            for k, v in d.items():\n",
        "                if category == \"results\" and k == \"step\":\n",
        "                    continue\n",
        "                self._wandb.log({category + \"/\" + k: v}, step=d[xkey])\n",
        "        if category == \"eval\" and self._save_csv:\n",
        "            keys = [\"step\", \"episode_reward\"]\n",
        "            self._eval.append(np.array([d[keys[0]], d[keys[1]]]))\n",
        "            pd.DataFrame(np.array(self._eval)).to_csv(\n",
        "                self._log_dir / \"eval.csv\", header=keys, index=None\n",
        "            )\n",
        "        if category != \"results\":\n",
        "            self._print(d, category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5bcc27e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# buffer\n",
        "class Buffer:\n",
        "    \"\"\"\n",
        "    Replay buffer for TD-MPC2 training. Based on torchrl.\n",
        "    Uses CUDA memory if available, and CPU memory otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        if sys.platform == \"darwin\":\n",
        "            self._device = torch.device(\"cpu\")\n",
        "        else:\n",
        "            self._device = torch.device(\"cuda\")\n",
        "        self._capacity = min(cfg.buffer_size, cfg.steps)\n",
        "        self._sampler = SliceSampler(\n",
        "            num_slices=self.cfg.batch_size,\n",
        "            end_key=None,\n",
        "            traj_key=\"episode\",\n",
        "            truncated_key=None,\n",
        "        )\n",
        "        self._batch_size = cfg.batch_size * (cfg.horizon + 1)\n",
        "        self._num_eps = 0\n",
        "\n",
        "    @property\n",
        "    def capacity(self):\n",
        "        \"\"\"Return the capacity of the buffer.\"\"\"\n",
        "        return self._capacity\n",
        "\n",
        "    @property\n",
        "    def num_eps(self):\n",
        "        \"\"\"Return the number of episodes in the buffer.\"\"\"\n",
        "        return self._num_eps\n",
        "\n",
        "    def _reserve_buffer(self, storage):\n",
        "        \"\"\"\n",
        "        Reserve a buffer with the given storage.\n",
        "        \"\"\"\n",
        "        return ReplayBuffer(\n",
        "            storage=storage,\n",
        "            sampler=self._sampler,\n",
        "            pin_memory=True,\n",
        "            prefetch=1,\n",
        "            batch_size=self._batch_size,\n",
        "        )\n",
        "\n",
        "    def _init(self, tds):\n",
        "        \"\"\"Initialize the replay buffer. Use the first episode to estimate storage requirements.\"\"\"\n",
        "        print(f\"Buffer capacity: {self._capacity:,}\")\n",
        "        if sys.platform == \"darwin\":\n",
        "            mem_free = 0\n",
        "        else:\n",
        "            mem_free, _ = torch.cuda.mem_get_info()\n",
        "        bytes_per_step = sum(\n",
        "            [\n",
        "                (\n",
        "                    v.numel() * v.element_size()\n",
        "                    if not isinstance(v, TensorDict)\n",
        "                    else sum([x.numel() * x.element_size() for x in v.values()])\n",
        "                )\n",
        "                for v in tds.values()\n",
        "            ]\n",
        "        ) / len(tds)\n",
        "        total_bytes = bytes_per_step * self._capacity\n",
        "        print(f\"Storage required: {total_bytes/1e9:.2f} GB\")\n",
        "        # Heuristic: decide whether to use CUDA or CPU memory\n",
        "        storage_device = \"cuda\" if 2.5 * total_bytes < mem_free else \"cpu\"\n",
        "        print(f\"Using {storage_device.upper()} memory for storage.\")\n",
        "        return self._reserve_buffer(\n",
        "            LazyTensorStorage(self._capacity, device=torch.device(storage_device))\n",
        "        )\n",
        "\n",
        "    def _to_device(self, *args, device=None):\n",
        "        if device is None:\n",
        "            device = self._device\n",
        "        return (\n",
        "            arg.to(device, non_blocking=True) if arg is not None else None\n",
        "            for arg in args\n",
        "        )\n",
        "\n",
        "    def _prepare_batch(self, td):\n",
        "        \"\"\"\n",
        "        Prepare a sampled batch for training (post-processing).\n",
        "        Expects `td` to be a TensorDict with batch size TxB.\n",
        "        \"\"\"\n",
        "        # add capacity to store mu adn std.\n",
        "        obs = td[\"obs\"]\n",
        "        action = td[\"action\"][1:]\n",
        "        mu = td[\"mu\"][1:]#\n",
        "        std = td[\"std\"][1:]#\n",
        "        reward = td[\"reward\"][1:].unsqueeze(-1)\n",
        "        task = td[\"task\"][0] if \"task\" in td.keys() else None\n",
        "        return self._to_device(obs, action, mu, std, reward, task)\n",
        "\n",
        "    def add(self, td):\n",
        "        \"\"\"Add an episode to the buffer.\"\"\"\n",
        "        td[\"episode\"] = torch.ones_like(td[\"reward\"], dtype=torch.int64) * self._num_eps\n",
        "\n",
        "        # FIX for HumanoidBench #\n",
        "        if len(td[\"episode\"]) <= self.cfg.horizon + 1:\n",
        "            return self._num_eps\n",
        "        ################################\n",
        "\n",
        "        if self._num_eps == 0:\n",
        "            self._buffer = self._init(td)\n",
        "        self._buffer.extend(td)\n",
        "        self._num_eps += 1\n",
        "        return self._num_eps\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Sample a batch of subsequences from the buffer.\"\"\"\n",
        "        td = self._buffer.sample().view(-1, self.cfg.horizon + 1).permute(1, 0)\n",
        "        return self._prepare_batch(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217c2099",
      "metadata": {},
      "outputs": [],
      "source": [
        "# config\n",
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        # environment\n",
        "        self.task = \"ms_packman\" # task name (or mt30/mt80 for multi-task training)\n",
        "        self.obs = \"rgb\" # observation type, must be one of `[rgb, state]` (default: rgb)\n",
        "        # evaluation\n",
        "        self.checkpoint = None\n",
        "        self.eval_episodes = 1\n",
        "        self.eval_pi = True\n",
        "        self.eval_value = True\n",
        "        self.eval_freq = 1000\n",
        "        # training\n",
        "        self.steps = 100000 # number of training/environment steps (default: 10M)\n",
        "        self.batch_size = 256\n",
        "        self.reward_coef = 0.1\n",
        "        self.value_coef = 0.1\n",
        "        self.consistency_coef = 20\n",
        "        self.rho = 0.5\n",
        "        self.lr = 3e-4\n",
        "        self.enc_lr_scale = 0.3\n",
        "        self.grad_clip_norm = 20\n",
        "        self.tau = 0.01\n",
        "        self.discount_denom = 5\n",
        "        self.discount_min = 0.95\n",
        "        self.discount_max = 0.995\n",
        "        self.buffer_size = 1_000_000\n",
        "        self.exp_name = \"default\"\n",
        "        # planning\n",
        "        self.mpc = True\n",
        "        self.iterations = 6\n",
        "        self.num_samples = 512\n",
        "        self.num_elites = 64\n",
        "        self.num_pi_trajs = 24\n",
        "        self.horizon = 10 # 15 # default 3\n",
        "        self.min_std = 0.05\n",
        "        self.max_std = 2\n",
        "        self.temperature = 0.5\n",
        "        # actor\n",
        "        self.actor_mode = \"residual\"\n",
        "        self.log_std_min = -10\n",
        "        self.log_std_max = 2\n",
        "        self.prior_coef = 1.0\n",
        "        self.scale_threshold = 2.0\n",
        "        self.entropy_coef = 1e-4\n",
        "        self.awac_lambda = 0.3333\n",
        "        self.exp_adv_min = 0.1\n",
        "        self.exp_adv_max = 10.0\n",
        "        # critic\n",
        "        self.num_bins = 101\n",
        "        self.vmin = -10\n",
        "        self.vmax = +10\n",
        "        # architecture\n",
        "        self.model_size = 5 # model size, must be one of `[1, 5, 19, 48, 317]` (default: 5)\n",
        "        self.num_enc_layers = 2\n",
        "        self.enc_dim = 256\n",
        "        self.num_channels = 32\n",
        "        self.mlp_dim = 512\n",
        "        self.latent_dim = 512\n",
        "        self.task_dim = 0\n",
        "        self.num_q = 5\n",
        "        self.dropout = 0.01\n",
        "        self.simnorm_dim = 8\n",
        "        # logging\n",
        "        self.wandb_project = \"tdmcp-square-test\"\n",
        "        self.wandb_entity = \"hirekatsu0523\"\n",
        "        self.wandb_silent = False\n",
        "        self.disable_wandb = False # いったんTrueにしておく\n",
        "        self.save_csv = True\n",
        "        # misc\n",
        "        self.save_video = True\n",
        "        self.save_agent = True\n",
        "        self.seed = 1\n",
        "        # convenience\n",
        "        self.work_dir = Path(\"./log1\")\n",
        "        self.multitask = False\n",
        "        self.tasks = None # Noneで良い\n",
        "        self.obs_shape = None\n",
        "        self.action_dim = None\n",
        "        self.episode_length = 300 # 良く分からないからとりあえず1000\n",
        "        self.action_dims = None # Noneで良い\n",
        "        self.episode_lengths = None # Noneで良い\n",
        "        self.seed_steps = 1000 # ランダムに行動するステップ数\n",
        "        self.bin_size = (self.vmax - self.vmin) / self.num_bins\n",
        "cfg = Config()\n",
        "MODEL_SIZE = {  # parameters (M)\n",
        "    1: {\n",
        "        \"enc_dim\": 256,\n",
        "        \"mlp_dim\": 384,\n",
        "        \"latent_dim\": 128,\n",
        "        \"num_enc_layers\": 2,\n",
        "        \"num_q\": 2,\n",
        "    },\n",
        "    5: {\"enc_dim\": 256, \"mlp_dim\": 512, \"latent_dim\": 512, \"num_enc_layers\": 2},\n",
        "    19: {\"enc_dim\": 1024, \"mlp_dim\": 1024, \"latent_dim\": 768, \"num_enc_layers\": 3},\n",
        "    48: {\"enc_dim\": 1792, \"mlp_dim\": 1792, \"latent_dim\": 768, \"num_enc_layers\": 4},\n",
        "    317: {\n",
        "        \"enc_dim\": 4096,\n",
        "        \"mlp_dim\": 4096,\n",
        "        \"latent_dim\": 1376,\n",
        "        \"num_enc_layers\": 5,\n",
        "        \"num_q\": 8,\n",
        "    },\n",
        "}\n",
        "# cfg.model_sizeに応じてcfgを更新\n",
        "cfg.__dict__.update(MODEL_SIZE[cfg.model_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "976e649a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action_dim: 9\n",
            "obs_shape: {'rgb': (3, 64, 64)}\n"
          ]
        }
      ],
      "source": [
        "# 環境の初期化\n",
        "NUM_ITER = 100_000  # 環境とのインタラクション回数の制限 ※変更しないでください\n",
        "set_seed(cfg.seed)\n",
        "env = make_env(max_steps=NUM_ITER)\n",
        "eval_env = make_env(seed=1234, max_steps=None)  # omnicampus上の環境と同じシード値で評価環境を作成\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "cfg.action_dim = env.action_space.n\n",
        "cfg.obs_shape = {\"rgb\":env.observation_space.shape}\n",
        "# actionは0~8の整数で指定\n",
        "print(\"action_dim:\", cfg.action_dim)\n",
        "print(\"obs_shape:\", cfg.obs_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a6b1b836",
      "metadata": {},
      "outputs": [],
      "source": [
        "# online trainer\n",
        "class OnlineTrainer():\n",
        "    \"\"\"Trainer class for single-task online TD-MPC2 training.\"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config, env, eval_env, agent: TDMPC2, buffer: Buffer, logger: Logger):\n",
        "        self.cfg = cfg\n",
        "        self.env = env\n",
        "        self.eval_env = eval_env\n",
        "        self.agent = agent\n",
        "        self.buffer = buffer\n",
        "        self.logger = logger\n",
        "        print(\"Architecture:\", self.agent.model)\n",
        "        print(\"Learnable parameters: {:,}\".format(self.agent.model.total_params))\n",
        "        self._step = 0\n",
        "        self._ep_idx = 0\n",
        "        self._start_time = time()\n",
        "\n",
        "    def common_metrics(self):\n",
        "        \"\"\"Return a dictionary of current metrics.\"\"\"\n",
        "        return dict(\n",
        "            step=self._step,\n",
        "            episode=self._ep_idx,\n",
        "            total_time=time() - self._start_time,\n",
        "        )\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Evaluate a TD-MPC2 agent.\"\"\"\n",
        "        ep_rewards, ep_successes = [], []\n",
        "        for i in range(self.cfg.eval_episodes):\n",
        "            obs, done, ep_reward, t = self.eval_env.reset(), False, 0, 0\n",
        "            if self.cfg.save_video:\n",
        "                self.logger.video.init(self.eval_env, enabled=(i == 0))\n",
        "            while not done:\n",
        "                action, _, _ = self.agent.act(obs, t0=t == 0, eval_mode=True)\n",
        "                obs, reward, done, truncated, _ = self.eval_env.step(action)\n",
        "                done = done or truncated\n",
        "                ep_reward += reward\n",
        "                t += 1\n",
        "                if self.cfg.save_video:\n",
        "                    self.logger.video.record(self.eval_env)\n",
        "            ep_rewards.append(ep_reward)\n",
        "            ep_successes.append(True)\n",
        "            if self.cfg.save_video:\n",
        "                # self.logger.video.save(self._step)\n",
        "                self.logger.video.save(self._step, key='results/video')\n",
        "        \n",
        "        if self.cfg.eval_pi:\n",
        "            # Evaluate nominal policy pi\n",
        "            ep_rewards_pi, ep_successes_pi = [], []\n",
        "            for i in range(self.cfg.eval_episodes):\n",
        "                obs, done, ep_reward, t = self.eval_env.reset(), False, 0, 0\n",
        "                while not done:\n",
        "                    action, _, _ = self.agent.act(obs, t0=t == 0, eval_mode=True, use_pi=True)\n",
        "                    obs, reward, done, truncated, _ = self.eval_env.step(action)\n",
        "                    done = done or truncated\n",
        "                    ep_reward += reward\n",
        "                    t += 1\n",
        "                ep_rewards_pi.append(ep_reward)\n",
        "                ep_successes_pi.append(True)\n",
        "            \n",
        "        return dict(\n",
        "            episode_reward=np.nanmean(ep_rewards),\n",
        "            episode_success=np.nanmean(ep_successes),\n",
        "            episode_reward_pi=np.nanmean(ep_rewards_pi) if self.cfg.eval_pi else np.nan,\n",
        "            episode_success_pi=np.nanmean(ep_successes_pi) if self.cfg.eval_pi else np.nan,\n",
        "        )\n",
        "\n",
        "    def eval_value(self, n_samples=100):\n",
        "        \"\"\"evaluate value approximation.\"\"\"\n",
        "        # MC value estimation\n",
        "        mc_ep_rewards = []\n",
        "        for i in range(n_samples):\n",
        "            obs, done, ep_reward, t = self.eval_env.reset(), False, 0, 0\n",
        "            while not done:\n",
        "                action, _, _ = self.agent.act(obs, t0=t == 0, eval_mode=True, use_pi=True)\n",
        "                obs, reward, done, truncated, _ = self.eval_env.step(action)\n",
        "                done = done or truncated\n",
        "                ep_reward += reward * self.agent.discount ** t\n",
        "                t += 1\n",
        "            mc_ep_rewards.append(ep_reward)\n",
        "\n",
        "        # Value function approximation\n",
        "        q_values = []\n",
        "        for i in range(n_samples):\n",
        "            obs, done, ep_reward, t = self.eval_env.reset(), False, 0, 0\n",
        "            \n",
        "            action, _, _ = self.agent.act(obs, t0=t == 0, eval_mode=True, use_pi=True)\n",
        "            task = None\n",
        "            # q_value = self.agent.model.Q(self.agent.model.encode(obs.to(self.agent.device), task), \n",
        "            q_value = self.agent.model.Q(self.agent.model.encode(torch.tensor(obs, device=self.agent.device), task),\n",
        "                                         action.to(self.agent.device), \n",
        "                                         task, return_type=\"avg\")\n",
        "            q_values.append(q_value.detach().cpu().numpy())\n",
        "        \n",
        "        return dict(\n",
        "            mc_value= np.nanmean(mc_ep_rewards),\n",
        "            q_value= np.nanmean(q_values),\n",
        "        )\n",
        "\n",
        "    def to_td(self, obs, action=None, mu=None, std=None, reward=None):\n",
        "        \"\"\"Creates a TensorDict for a new episode.\"\"\"\n",
        "        if isinstance(obs, dict):\n",
        "            obs = TensorDict(obs, batch_size=(), device=\"cpu\")\n",
        "        else:\n",
        "            obs = torch.tensor(obs, device=\"cpu\")\n",
        "            obs = obs.unsqueeze(0).cpu()\n",
        "        if action is None:\n",
        "            action = torch.full_like(self.env.rand_act(), float(\"nan\"))\n",
        "        if mu is None:\n",
        "            mu = torch.full_like(action, float(\"nan\"))\n",
        "        if std is None:\n",
        "            std = torch.full_like(action, float(\"nan\"))\n",
        "        if reward is None:\n",
        "            reward = torch.tensor(float(\"nan\"))\n",
        "        # rewardがfloatならtorch.tensorに変換\n",
        "        if isinstance(reward, float):\n",
        "            reward = torch.tensor(reward)\n",
        "        td = TensorDict(\n",
        "            dict(\n",
        "                obs=obs,\n",
        "                action=action.unsqueeze(0) if len(action.shape) == 1 else action,\n",
        "                mu=mu.unsqueeze(0),\n",
        "                std=std.unsqueeze(0),\n",
        "                reward=reward.unsqueeze(0),\n",
        "            ),\n",
        "            batch_size=(1,),\n",
        "        )\n",
        "        return td\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train a TD-MPC2 agent.\"\"\"\n",
        "        train_metrics, done, eval_next = {}, True, True\n",
        "\n",
        "        while self._step <= self.cfg.steps:\n",
        "            # Evaluate agent periodically\n",
        "            if self._step % self.cfg.eval_freq == 0:\n",
        "                eval_next = True\n",
        "\n",
        "            # Reset environment\n",
        "            if done:\n",
        "                if eval_next:\n",
        "                    eval_metrics = self.eval()\n",
        "\n",
        "                    if self.cfg.eval_value:\n",
        "                        eval_metrics.update(self.eval_value())\n",
        "\n",
        "                    eval_metrics.update(self.common_metrics())\n",
        "                    self.logger.log(eval_metrics, \"eval\")\n",
        "                    eval_next = False\n",
        "\n",
        "                if self._step > 0:\n",
        "                    train_metrics.update(\n",
        "                        episode_reward=torch.tensor(\n",
        "                            [td[\"reward\"] for td in self._tds[1:]]\n",
        "                        ).sum(),\n",
        "                        episode_success=True,\n",
        "                    )\n",
        "                    train_metrics.update(self.common_metrics())\n",
        "\n",
        "                    results_metrics = {\n",
        "                        'return': train_metrics['episode_reward'],\n",
        "                        'episode_length': len(self._tds[1:]),\n",
        "                        'success': train_metrics['episode_success'],\n",
        "                        'step': self._step,}\n",
        "\n",
        "                    self.logger.log(train_metrics, \"train\")\n",
        "                    self.logger.log(results_metrics, \"results\")\n",
        "                    self._ep_idx = self.buffer.add(torch.cat(self._tds))\n",
        "\n",
        "                obs = self.env.reset()\n",
        "                self._tds = [self.to_td(obs)]\n",
        "\n",
        "            # Collect experience\n",
        "            if self._step > self.cfg.seed_steps:\n",
        "                t0 = len(self._tds) == 1\n",
        "                action, mu, std = self.agent.act(obs, t0=t0)\n",
        "            else:\n",
        "                action = self.env.rand_act()\n",
        "                mu, std = action.detach().clone(), torch.full_like(action, math.exp(self.cfg.log_std_max)) # torch.full_like(action, float('nan')), torch.full_like(action, float('nan')) #  # noqa\n",
        "            obs, reward, done, truncated, _ = self.env.step(action)\n",
        "            done = done or truncated\n",
        "            self._tds.append(self.to_td(obs, action, mu, std, reward))\n",
        "\n",
        "            # Update agent\n",
        "            if self._step >= self.cfg.seed_steps:\n",
        "                if self._step == self.cfg.seed_steps:\n",
        "                    num_updates = self.cfg.seed_steps\n",
        "                    print(\"Pretraining agent on seed data...\")\n",
        "                else:\n",
        "                    num_updates = 1\n",
        "                for _ in range(num_updates):\n",
        "                    _train_metrics = self.agent.update(self.buffer) # 少なくとも1回はdoneをする前に，updateを呼び出すとエラー self.seed_stepsを大きくすると良い\n",
        "                train_metrics.update(_train_metrics)\n",
        "\n",
        "            self._step += 1\n",
        "\n",
        "        self.logger.finish(self.agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "244d6a8b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "module 'numpy' has no attribute 'bool8'\n",
            "module 'numpy' has no attribute 'bool8'\n"
          ]
        }
      ],
      "source": [
        "# エラー回避\n",
        "try:\n",
        "    env.reset()\n",
        "    env.step(torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "try:\n",
        "    eval_env.reset()\n",
        "    eval_env.step(torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b35d8f-5339-42c4-90a3-581d17a3960d",
      "metadata": {
        "id": "a8b35d8f-5339-42c4-90a3-581d17a3960d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Work dir: log1\n",
            "------------------------------------\n",
            "  Task:          \n",
            "  Steps:         \n",
            "  Observations:  \n",
            "  Actions:       \n",
            "  Experiment:    \n",
            "------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hirek\\AppData\\Local\\Temp\\ipykernel_44632\\3277046652.py:10: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.combine_state_for_ensemble` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.stack_module_state` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
            "  fn, params, _ = combine_state_for_ensemble(modules)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhirekatsu0523\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>log1\\wandb\\run-20250223_161034-axgp2yxi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hirekatsu0523/tdmcp-square-test/runs/axgp2yxi' target=\"_blank\">ms_packman.tdmpc.default.1</a></strong> to <a href='https://wandb.ai/hirekatsu0523/tdmcp-square-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/hirekatsu0523/tdmcp-square-test' target=\"_blank\">https://wandb.ai/hirekatsu0523/tdmcp-square-test</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/hirekatsu0523/tdmcp-square-test/runs/axgp2yxi' target=\"_blank\">https://wandb.ai/hirekatsu0523/tdmcp-square-test/runs/axgp2yxi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logs will be synced with wandb.\n",
            "Architecture: WorldModel(\n",
            "  (_encoder): ModuleDict(\n",
            "    (rgb): Sequential(\n",
            "      (0): ShiftAug()\n",
            "      (1): PixelPreprocess()\n",
            "      (2): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
            "      (7): ReLU(inplace=True)\n",
            "      (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "      (9): Flatten(start_dim=1, end_dim=-1)\n",
            "      (10): SimNorm(dim=8)\n",
            "    )\n",
            "  )\n",
            "  (_dynamics): Sequential(\n",
            "    (0): NormedLinear(in_features=521, out_features=512, bias=True, act=Mish)\n",
            "    (1): NormedLinear(in_features=512, out_features=512, bias=True, act=Mish)\n",
            "    (2): NormedLinear(in_features=512, out_features=512, bias=True, act=SimNorm)\n",
            "  )\n",
            "  (_reward): Sequential(\n",
            "    (0): NormedLinear(in_features=521, out_features=512, bias=True, act=Mish)\n",
            "    (1): NormedLinear(in_features=512, out_features=512, bias=True, act=Mish)\n",
            "    (2): Linear(in_features=512, out_features=101, bias=True)\n",
            "  )\n",
            "  (_pi): Sequential(\n",
            "    (0): NormedLinear(in_features=512, out_features=512, bias=True, act=Mish)\n",
            "    (1): NormedLinear(in_features=512, out_features=512, bias=True, act=Mish)\n",
            "    (2): Linear(in_features=512, out_features=18, bias=True)\n",
            "  )\n",
            "  (_Qs): Vectorized ModuleList(\n",
            "    (0-4): 5 x Sequential(\n",
            "      (0): NormedLinear(in_features=521, out_features=512, bias=True, dropout=0.01, act=Mish)\n",
            "      (1): NormedLinear(in_features=512, out_features=512, bias=True, act=Mish)\n",
            "      (2): Linear(in_features=512, out_features=101, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (_target_Qs): Vectorized ModuleList(\n",
            "    (0-4): 5 x Sequential(\n",
            "      (0): NormedLinear(in_features=521, out_features=512, bias=True, dropout=0.01, act=Mish)\n",
            "      (1): NormedLinear(in_features=512, out_features=512, bias=True, act=Mish)\n",
            "      (2): Linear(in_features=512, out_features=101, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Learnable parameters: 4,883,792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\SourceCode\\VScode\\wm_compe\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " eval             E: 0                     I: 0                     R: 280.0                 S: 1.0                   T: 0:00:47            \n",
            " train            E: 0                     I: 92                    R: 230.0                 S: 1.0                   T: 0:00:47            \n",
            "Buffer capacity: 100,000\n",
            "Storage required: 1.24 GB\n",
            "Using CUDA memory for storage.\n",
            " train            E: 1                     I: 202                   R: 290.0                 S: 1.0                   T: 0:00:47            \n",
            " train            E: 2                     I: 333                   R: 270.0                 S: 1.0                   T: 0:00:47            \n",
            " train            E: 3                     I: 477                   R: 370.0                 S: 1.0                   T: 0:00:48            \n",
            " train            E: 4                     I: 663                   R: 580.0                 S: 1.0                   T: 0:00:48            \n",
            " train            E: 5                     I: 799                   R: 340.0                 S: 1.0                   T: 0:00:48            \n",
            " train            E: 6                     I: 912                   R: 250.0                 S: 1.0                   T: 0:00:48            \n",
            "Pretraining agent on seed data...\n",
            " eval             E: 7                     I: 1,018                 R: 440.0                 S: 1.0                   T: 0:02:42            \n",
            " train            E: 7                     I: 1,018                 R: 190.0                 S: 1.0                   T: 0:02:42            \n",
            " train            E: 8                     I: 1,137                 R: 420.0                 S: 1.0                   T: 0:02:56            \n",
            " train            E: 9                     I: 1,295                 R: 250.0                 S: 1.0                   T: 0:03:16            \n",
            " train            E: 10                    I: 1,424                 R: 480.0                 S: 1.0                   T: 0:03:31            \n",
            " train            E: 11                    I: 1,567                 R: 440.0                 S: 1.0                   T: 0:03:48            \n",
            " train            E: 12                    I: 1,758                 R: 560.0                 S: 1.0                   T: 0:04:11            \n",
            " train            E: 13                    I: 1,940                 R: 910.0                 S: 1.0                   T: 0:04:35            \n",
            " eval             E: 14                    I: 2,138                 R: 710.0                 S: 1.0                   T: 0:05:57            \n",
            " train            E: 14                    I: 2,138                 R: 870.0                 S: 1.0                   T: 0:05:57            \n",
            " train            E: 15                    I: 2,305                 R: 710.0                 S: 1.0                   T: 0:06:18            \n",
            " train            E: 16                    I: 2,389                 R: 200.0                 S: 1.0                   T: 0:06:28            \n",
            " train            E: 17                    I: 2,496                 R: 330.0                 S: 1.0                   T: 0:06:41            \n",
            " train            E: 18                    I: 2,590                 R: 350.0                 S: 1.0                   T: 0:06:53            \n",
            " train            E: 19                    I: 2,725                 R: 350.0                 S: 1.0                   T: 0:07:09            \n",
            " train            E: 20                    I: 2,861                 R: 510.0                 S: 1.0                   T: 0:07:26            \n",
            " train            E: 21                    I: 2,991                 R: 470.0                 S: 1.0                   T: 0:07:43            \n",
            " eval             E: 22                    I: 3,146                 R: 1190.0                S: 1.0                   T: 0:08:53            \n",
            " train            E: 22                    I: 3,146                 R: 680.0                 S: 1.0                   T: 0:08:53            \n",
            " train            E: 23                    I: 3,252                 R: 290.0                 S: 1.0                   T: 0:09:05            \n",
            " train            E: 24                    I: 3,350                 R: 310.0                 S: 1.0                   T: 0:09:17            \n",
            " train            E: 25                    I: 3,537                 R: 1100.0                S: 1.0                   T: 0:09:41            \n",
            " train            E: 26                    I: 3,656                 R: 430.0                 S: 1.0                   T: 0:09:56            \n",
            " train            E: 27                    I: 3,768                 R: 400.0                 S: 1.0                   T: 0:10:10            \n",
            " eval             E: 28                    I: 4,031                 R: 350.0                 S: 1.0                   T: 0:11:35            \n",
            " train            E: 28                    I: 4,031                 R: 1280.0                S: 1.0                   T: 0:11:35            \n",
            " train            E: 29                    I: 4,127                 R: 280.0                 S: 1.0                   T: 0:11:46            \n",
            " train            E: 30                    I: 4,266                 R: 570.0                 S: 1.0                   T: 0:12:01            \n",
            " train            E: 31                    I: 4,359                 R: 340.0                 S: 1.0                   T: 0:12:12            \n",
            " train            E: 32                    I: 4,485                 R: 410.0                 S: 1.0                   T: 0:12:26            \n",
            " train            E: 33                    I: 4,588                 R: 280.0                 S: 1.0                   T: 0:12:37            \n",
            " train            E: 34                    I: 4,699                 R: 370.0                 S: 1.0                   T: 0:12:49            \n",
            " train            E: 35                    I: 4,833                 R: 360.0                 S: 1.0                   T: 0:13:04            \n",
            " train            E: 36                    I: 4,952                 R: 360.0                 S: 1.0                   T: 0:13:18            \n",
            " eval             E: 37                    I: 5,064                 R: 1090.0                S: 1.0                   T: 0:14:27            \n",
            " train            E: 37                    I: 5,064                 R: 300.0                 S: 1.0                   T: 0:14:27            \n",
            " train            E: 38                    I: 5,264                 R: 940.0                 S: 1.0                   T: 0:14:50            \n",
            " train            E: 39                    I: 5,428                 R: 560.0                 S: 1.0                   T: 0:15:08            \n",
            " train            E: 40                    I: 5,586                 R: 700.0                 S: 1.0                   T: 0:15:25            \n",
            " train            E: 41                    I: 5,767                 R: 720.0                 S: 1.0                   T: 0:15:46            \n",
            " train            E: 42                    I: 5,870                 R: 420.0                 S: 1.0                   T: 0:15:57            \n",
            " train            E: 43                    I: 5,962                 R: 200.0                 S: 1.0                   T: 0:16:07            \n",
            " eval             E: 44                    I: 6,153                 R: 290.0                 S: 1.0                   T: 0:17:19            \n",
            " train            E: 44                    I: 6,153                 R: 880.0                 S: 1.0                   T: 0:17:19            \n",
            " train            E: 45                    I: 6,263                 R: 290.0                 S: 1.0                   T: 0:17:31            \n",
            " train            E: 46                    I: 6,480                 R: 1070.0                S: 1.0                   T: 0:17:55            \n",
            " train            E: 47                    I: 6,629                 R: 410.0                 S: 1.0                   T: 0:18:12            \n",
            " train            E: 48                    I: 6,790                 R: 470.0                 S: 1.0                   T: 0:18:30            \n",
            " train            E: 49                    I: 6,893                 R: 370.0                 S: 1.0                   T: 0:18:41            \n",
            " eval             E: 50                    I: 7,035                 R: 500.0                 S: 1.0                   T: 0:19:48            \n",
            " train            E: 50                    I: 7,035                 R: 450.0                 S: 1.0                   T: 0:19:48            \n",
            " train            E: 51                    I: 7,149                 R: 360.0                 S: 1.0                   T: 0:20:01            \n",
            " train            E: 52                    I: 7,313                 R: 690.0                 S: 1.0                   T: 0:20:19            \n",
            " train            E: 53                    I: 7,489                 R: 1090.0                S: 1.0                   T: 0:20:39            \n",
            " train            E: 54                    I: 7,624                 R: 380.0                 S: 1.0                   T: 0:20:54            \n",
            " train            E: 55                    I: 7,838                 R: 920.0                 S: 1.0                   T: 0:21:18            \n",
            " eval             E: 56                    I: 8,044                 R: 480.0                 S: 1.0                   T: 0:22:30            \n",
            " train            E: 56                    I: 8,044                 R: 950.0                 S: 1.0                   T: 0:22:30            \n",
            " train            E: 57                    I: 8,245                 R: 900.0                 S: 1.0                   T: 0:22:52            \n",
            " train            E: 58                    I: 8,393                 R: 500.0                 S: 1.0                   T: 0:23:09            \n",
            " train            E: 59                    I: 8,564                 R: 750.0                 S: 1.0                   T: 0:23:28            \n",
            " train            E: 60                    I: 8,691                 R: 380.0                 S: 1.0                   T: 0:23:42            \n",
            " train            E: 61                    I: 8,821                 R: 470.0                 S: 1.0                   T: 0:23:56            \n",
            " train            E: 62                    I: 8,920                 R: 230.0                 S: 1.0                   T: 0:24:07            \n",
            " eval             E: 63                    I: 9,026                 R: 330.0                 S: 1.0                   T: 0:25:06            \n",
            " train            E: 63                    I: 9,026                 R: 310.0                 S: 1.0                   T: 0:25:06            \n",
            " train            E: 64                    I: 9,206                 R: 1040.0                S: 1.0                   T: 0:25:26            \n",
            " train            E: 65                    I: 9,344                 R: 460.0                 S: 1.0                   T: 0:25:42            \n",
            " train            E: 66                    I: 9,445                 R: 350.0                 S: 1.0                   T: 0:25:53            \n",
            " train            E: 67                    I: 9,605                 R: 670.0                 S: 1.0                   T: 0:26:11            \n",
            " train            E: 68                    I: 9,753                 R: 520.0                 S: 1.0                   T: 0:26:28            \n",
            " train            E: 69                    I: 9,881                 R: 370.0                 S: 1.0                   T: 0:26:42            \n",
            " train            E: 70                    I: 9,990                 R: 450.0                 S: 1.0                   T: 0:26:54            \n",
            " eval             E: 71                    I: 10,093                R: 560.0                 S: 1.0                   T: 0:28:01            \n",
            " train            E: 71                    I: 10,093                R: 290.0                 S: 1.0                   T: 0:28:01            \n",
            " train            E: 72                    I: 10,280                R: 950.0                 S: 1.0                   T: 0:28:22            \n",
            " train            E: 73                    I: 10,391                R: 360.0                 S: 1.0                   T: 0:28:35            \n",
            " train            E: 74                    I: 10,512                R: 300.0                 S: 1.0                   T: 0:28:50            \n",
            " train            E: 75                    I: 10,651                R: 470.0                 S: 1.0                   T: 0:29:06            \n",
            " train            E: 76                    I: 10,767                R: 350.0                 S: 1.0                   T: 0:29:19            \n",
            " train            E: 77                    I: 10,892                R: 400.0                 S: 1.0                   T: 0:29:33            \n",
            " eval             E: 78                    I: 11,151                R: 430.0                 S: 1.0                   T: 0:30:57            \n",
            " train            E: 78                    I: 11,151                R: 1950.0                S: 1.0                   T: 0:30:57            \n",
            " train            E: 79                    I: 11,258                R: 170.0                 S: 1.0                   T: 0:31:09            \n",
            " train            E: 80                    I: 11,388                R: 330.0                 S: 1.0                   T: 0:31:25            \n",
            " train            E: 81                    I: 11,530                R: 790.0                 S: 1.0                   T: 0:31:41            \n",
            " train            E: 82                    I: 11,671                R: 440.0                 S: 1.0                   T: 0:31:57            \n",
            " train            E: 83                    I: 11,792                R: 360.0                 S: 1.0                   T: 0:32:11            \n",
            " train            E: 84                    I: 11,992                R: 1360.0                S: 1.0                   T: 0:32:33            \n",
            " eval             E: 85                    I: 12,107                R: 660.0                 S: 1.0                   T: 0:33:36            \n",
            " train            E: 85                    I: 12,107                R: 370.0                 S: 1.0                   T: 0:33:36            \n",
            " train            E: 86                    I: 12,364                R: 1160.0                S: 1.0                   T: 0:34:07            \n",
            " train            E: 87                    I: 12,483                R: 370.0                 S: 1.0                   T: 0:34:21            \n",
            " train            E: 88                    I: 12,664                R: 1050.0                S: 1.0                   T: 0:34:41            \n",
            " train            E: 89                    I: 12,845                R: 620.0                 S: 1.0                   T: 0:35:02            \n",
            " train            E: 90                    I: 12,934                R: 200.0                 S: 1.0                   T: 0:35:12            \n",
            " eval             E: 91                    I: 13,053                R: 320.0                 S: 1.0                   T: 0:36:21            \n",
            " train            E: 91                    I: 13,053                R: 310.0                 S: 1.0                   T: 0:36:21            \n",
            " train            E: 92                    I: 13,173                R: 340.0                 S: 1.0                   T: 0:36:35            \n",
            " train            E: 93                    I: 13,317                R: 680.0                 S: 1.0                   T: 0:36:52            \n",
            " train            E: 94                    I: 13,468                R: 550.0                 S: 1.0                   T: 0:37:09            \n",
            " train            E: 95                    I: 13,664                R: 880.0                 S: 1.0                   T: 0:37:31            \n",
            " train            E: 96                    I: 13,796                R: 460.0                 S: 1.0                   T: 0:37:47            \n",
            " train            E: 97                    I: 13,904                R: 380.0                 S: 1.0                   T: 0:38:00            \n",
            " eval             E: 98                    I: 14,073                R: 1030.0                S: 1.0                   T: 0:39:12            \n",
            " train            E: 98                    I: 14,073                R: 670.0                 S: 1.0                   T: 0:39:12            \n",
            " train            E: 99                    I: 14,161                R: 210.0                 S: 1.0                   T: 0:39:23            \n",
            " train            E: 100                   I: 14,304                R: 400.0                 S: 1.0                   T: 0:39:39            \n",
            " train            E: 101                   I: 14,415                R: 390.0                 S: 1.0                   T: 0:39:52            \n",
            " train            E: 102                   I: 14,516                R: 280.0                 S: 1.0                   T: 0:40:04            \n",
            " train            E: 103                   I: 14,610                R: 220.0                 S: 1.0                   T: 0:40:14            \n",
            " train            E: 104                   I: 14,734                R: 300.0                 S: 1.0                   T: 0:40:29            \n",
            " train            E: 105                   I: 14,882                R: 660.0                 S: 1.0                   T: 0:40:45            \n",
            " eval             E: 106                   I: 15,006                R: 420.0                 S: 1.0                   T: 0:41:54            \n",
            " train            E: 106                   I: 15,006                R: 470.0                 S: 1.0                   T: 0:41:54            \n",
            " train            E: 107                   I: 15,145                R: 450.0                 S: 1.0                   T: 0:42:10            \n",
            " train            E: 108                   I: 15,237                R: 270.0                 S: 1.0                   T: 0:42:20            \n",
            " train            E: 109                   I: 15,359                R: 460.0                 S: 1.0                   T: 0:42:34            \n",
            " train            E: 110                   I: 15,589                R: 890.0                 S: 1.0                   T: 0:43:01            \n",
            " train            E: 111                   I: 15,768                R: 1410.0                S: 1.0                   T: 0:43:23            \n",
            " train            E: 112                   I: 15,893                R: 350.0                 S: 1.0                   T: 0:43:38            \n",
            " eval             E: 113                   I: 16,030                R: 680.0                 S: 1.0                   T: 0:44:49            \n",
            " train            E: 113                   I: 16,030                R: 440.0                 S: 1.0                   T: 0:44:49            \n",
            " train            E: 114                   I: 16,144                R: 440.0                 S: 1.0                   T: 0:45:02            \n",
            " train            E: 115                   I: 16,303                R: 780.0                 S: 1.0                   T: 0:45:20            \n",
            " train            E: 116                   I: 16,483                R: 640.0                 S: 1.0                   T: 0:45:40            \n",
            " train            E: 117                   I: 16,653                R: 620.0                 S: 1.0                   T: 0:45:59            \n",
            " train            E: 118                   I: 16,751                R: 320.0                 S: 1.0                   T: 0:46:11            \n",
            " train            E: 119                   I: 16,928                R: 870.0                 S: 1.0                   T: 0:46:32            \n",
            " eval             E: 120                   I: 17,079                R: 670.0                 S: 1.0                   T: 0:47:43            \n",
            " train            E: 120                   I: 17,079                R: 520.0                 S: 1.0                   T: 0:47:43            \n",
            " train            E: 121                   I: 17,210                R: 510.0                 S: 1.0                   T: 0:47:58            \n",
            " train            E: 122                   I: 17,331                R: 580.0                 S: 1.0                   T: 0:48:12            \n",
            " train            E: 123                   I: 17,460                R: 510.0                 S: 1.0                   T: 0:48:27            \n",
            " train            E: 124                   I: 17,574                R: 330.0                 S: 1.0                   T: 0:48:40            \n",
            " train            E: 125                   I: 17,706                R: 400.0                 S: 1.0                   T: 0:48:55            \n",
            " train            E: 126                   I: 17,859                R: 690.0                 S: 1.0                   T: 0:49:12            \n",
            " train            E: 127                   I: 17,981                R: 400.0                 S: 1.0                   T: 0:49:27            \n",
            " eval             E: 128                   I: 18,083                R: 1140.0                S: 1.0                   T: 0:50:36            \n",
            " train            E: 128                   I: 18,083                R: 320.0                 S: 1.0                   T: 0:50:36            \n",
            " train            E: 129                   I: 18,213                R: 410.0                 S: 1.0                   T: 0:50:51            \n",
            " train            E: 130                   I: 18,328                R: 420.0                 S: 1.0                   T: 0:51:05            \n",
            " train            E: 131                   I: 18,460                R: 490.0                 S: 1.0                   T: 0:51:20            \n",
            " train            E: 132                   I: 18,653                R: 660.0                 S: 1.0                   T: 0:51:43            \n",
            " train            E: 133                   I: 18,766                R: 290.0                 S: 1.0                   T: 0:51:56            \n",
            " train            E: 134                   I: 18,882                R: 460.0                 S: 1.0                   T: 0:52:10            \n",
            " eval             E: 135                   I: 19,003                R: 260.0                 S: 1.0                   T: 0:53:14            \n",
            " train            E: 135                   I: 19,003                R: 380.0                 S: 1.0                   T: 0:53:14            \n",
            " train            E: 136                   I: 19,262                R: 1580.0                S: 1.0                   T: 0:53:44            \n",
            " train            E: 137                   I: 19,370                R: 230.0                 S: 1.0                   T: 0:53:57            \n",
            " train            E: 138                   I: 19,487                R: 320.0                 S: 1.0                   T: 0:54:11            \n",
            " train            E: 139                   I: 19,641                R: 460.0                 S: 1.0                   T: 0:54:28            \n",
            " train            E: 140                   I: 19,818                R: 730.0                 S: 1.0                   T: 0:54:48            \n",
            " train            E: 141                   I: 19,982                R: 750.0                 S: 1.0                   T: 0:55:08            \n",
            " eval             E: 142                   I: 20,140                R: 690.0                 S: 1.0                   T: 0:56:23            \n",
            " train            E: 142                   I: 20,140                R: 650.0                 S: 1.0                   T: 0:56:23            \n",
            " train            E: 143                   I: 20,229                R: 210.0                 S: 1.0                   T: 0:56:33            \n",
            " train            E: 144                   I: 20,336                R: 300.0                 S: 1.0                   T: 0:56:45            \n",
            " train            E: 145                   I: 20,531                R: 1600.0                S: 1.0                   T: 0:57:08            \n",
            " train            E: 146                   I: 20,666                R: 290.0                 S: 1.0                   T: 0:57:24            \n",
            " train            E: 147                   I: 20,813                R: 490.0                 S: 1.0                   T: 0:57:41            \n",
            " train            E: 148                   I: 20,932                R: 320.0                 S: 1.0                   T: 0:57:55            \n",
            " eval             E: 149                   I: 21,110                R: 400.0                 S: 1.0                   T: 0:59:08            \n",
            " train            E: 149                   I: 21,110                R: 950.0                 S: 1.0                   T: 0:59:08            \n",
            " train            E: 150                   I: 21,205                R: 250.0                 S: 1.0                   T: 0:59:20            \n",
            " train            E: 151                   I: 21,354                R: 380.0                 S: 1.0                   T: 0:59:37            \n",
            " train            E: 152                   I: 21,572                R: 1420.0                S: 1.0                   T: 1:00:03            \n",
            " train            E: 153                   I: 21,670                R: 280.0                 S: 1.0                   T: 1:00:15            \n",
            " train            E: 154                   I: 21,837                R: 500.0                 S: 1.0                   T: 1:00:33            \n",
            " train            E: 155                   I: 21,968                R: 390.0                 S: 1.0                   T: 1:00:48            \n",
            " eval             E: 156                   I: 22,147                R: 290.0                 S: 1.0                   T: 1:01:57            \n",
            " train            E: 156                   I: 22,147                R: 990.0                 S: 1.0                   T: 1:01:57            \n",
            " train            E: 157                   I: 22,263                R: 510.0                 S: 1.0                   T: 1:02:10            \n",
            " train            E: 158                   I: 22,407                R: 410.0                 S: 1.0                   T: 1:02:26            \n",
            " train            E: 159                   I: 22,568                R: 650.0                 S: 1.0                   T: 1:02:44            \n",
            " train            E: 160                   I: 22,764                R: 750.0                 S: 1.0                   T: 1:03:06            \n",
            " train            E: 161                   I: 22,899                R: 340.0                 S: 1.0                   T: 1:03:22            \n",
            " eval             E: 162                   I: 23,008                R: 460.0                 S: 1.0                   T: 1:04:25            \n",
            " train            E: 162                   I: 23,008                R: 280.0                 S: 1.0                   T: 1:04:25            \n",
            " train            E: 163                   I: 23,227                R: 860.0                 S: 1.0                   T: 1:04:50            \n",
            " train            E: 164                   I: 23,317                R: 300.0                 S: 1.0                   T: 1:05:00            \n",
            " train            E: 165                   I: 23,429                R: 340.0                 S: 1.0                   T: 1:05:12            \n",
            " train            E: 166                   I: 23,500                R: 200.0                 S: 1.0                   T: 1:05:20            \n",
            " train            E: 167                   I: 23,606                R: 370.0                 S: 1.0                   T: 1:05:31            \n",
            " train            E: 168                   I: 23,802                R: 1110.0                S: 1.0                   T: 1:05:53            \n",
            " train            E: 169                   I: 23,910                R: 310.0                 S: 1.0                   T: 1:06:04            \n",
            " eval             E: 170                   I: 24,170                R: 350.0                 S: 1.0                   T: 1:07:21            \n",
            " train            E: 170                   I: 24,170                R: 1790.0                S: 1.0                   T: 1:07:21            \n",
            " train            E: 171                   I: 24,288                R: 390.0                 S: 1.0                   T: 1:07:35            \n",
            " train            E: 172                   I: 24,397                R: 300.0                 S: 1.0                   T: 1:07:47            \n",
            " train            E: 173                   I: 24,519                R: 390.0                 S: 1.0                   T: 1:08:01            \n",
            " train            E: 174                   I: 24,662                R: 610.0                 S: 1.0                   T: 1:08:17            \n",
            " train            E: 175                   I: 24,789                R: 440.0                 S: 1.0                   T: 1:08:31            \n",
            " eval             E: 176                   I: 25,045                R: 1450.0                S: 1.0                   T: 1:09:45            \n",
            " train            E: 176                   I: 25,045                R: 1430.0                S: 1.0                   T: 1:09:45            \n",
            " train            E: 177                   I: 25,245                R: 920.0                 S: 1.0                   T: 1:10:08            \n",
            " train            E: 178                   I: 25,354                R: 340.0                 S: 1.0                   T: 1:10:21            \n",
            " train            E: 179                   I: 25,554                R: 1200.0                S: 1.0                   T: 1:10:43            \n",
            " train            E: 180                   I: 25,712                R: 540.0                 S: 1.0                   T: 1:11:00            \n",
            " train            E: 181                   I: 25,862                R: 700.0                 S: 1.0                   T: 1:11:17            \n",
            " train            E: 182                   I: 25,984                R: 430.0                 S: 1.0                   T: 1:11:32            \n",
            " eval             E: 183                   I: 26,120                R: 490.0                 S: 1.0                   T: 1:12:40            \n",
            " train            E: 183                   I: 26,120                R: 380.0                 S: 1.0                   T: 1:12:40            \n",
            " train            E: 184                   I: 26,255                R: 510.0                 S: 1.0                   T: 1:12:56            \n",
            " train            E: 185                   I: 26,394                R: 530.0                 S: 1.0                   T: 1:13:13            \n",
            " train            E: 186                   I: 26,515                R: 300.0                 S: 1.0                   T: 1:13:27            \n",
            " train            E: 187                   I: 26,710                R: 800.0                 S: 1.0                   T: 1:13:51            \n",
            " train            E: 188                   I: 26,922                R: 1080.0                S: 1.0                   T: 1:14:16            \n",
            " eval             E: 189                   I: 27,040                R: 440.0                 S: 1.0                   T: 1:15:20            \n",
            " train            E: 189                   I: 27,040                R: 460.0                 S: 1.0                   T: 1:15:20            \n",
            " train            E: 190                   I: 27,203                R: 770.0                 S: 1.0                   T: 1:15:40            \n",
            " train            E: 191                   I: 27,338                R: 450.0                 S: 1.0                   T: 1:15:56            \n",
            " train            E: 192                   I: 27,497                R: 700.0                 S: 1.0                   T: 1:16:14            \n",
            " train            E: 193                   I: 27,631                R: 380.0                 S: 1.0                   T: 1:16:29            \n",
            " train            E: 194                   I: 27,797                R: 610.0                 S: 1.0                   T: 1:16:50            \n",
            " train            E: 195                   I: 27,901                R: 290.0                 S: 1.0                   T: 1:17:02            \n",
            " eval             E: 196                   I: 28,021                R: 650.0                 S: 1.0                   T: 1:18:04            \n",
            " train            E: 196                   I: 28,021                R: 410.0                 S: 1.0                   T: 1:18:04            \n",
            " train            E: 197                   I: 28,180                R: 790.0                 S: 1.0                   T: 1:18:24            \n",
            " train            E: 198                   I: 28,307                R: 340.0                 S: 1.0                   T: 1:18:39            \n",
            " train            E: 199                   I: 28,434                R: 430.0                 S: 1.0                   T: 1:18:54            \n",
            " train            E: 200                   I: 28,560                R: 420.0                 S: 1.0                   T: 1:19:09            \n",
            " train            E: 201                   I: 28,670                R: 300.0                 S: 1.0                   T: 1:19:21            \n",
            " train            E: 202                   I: 28,782                R: 400.0                 S: 1.0                   T: 1:19:34            \n",
            " train            E: 203                   I: 28,903                R: 400.0                 S: 1.0                   T: 1:19:48            \n",
            " eval             E: 204                   I: 29,010                R: 380.0                 S: 1.0                   T: 1:20:50            \n",
            " train            E: 204                   I: 29,010                R: 380.0                 S: 1.0                   T: 1:20:50            \n",
            " train            E: 205                   I: 29,118                R: 400.0                 S: 1.0                   T: 1:21:03            \n",
            " train            E: 206                   I: 29,217                R: 250.0                 S: 1.0                   T: 1:21:14            \n",
            " train            E: 207                   I: 29,301                R: 310.0                 S: 1.0                   T: 1:21:24            \n",
            " train            E: 208                   I: 29,431                R: 400.0                 S: 1.0                   T: 1:21:40            \n",
            " train            E: 209                   I: 29,544                R: 290.0                 S: 1.0                   T: 1:21:54            \n",
            " train            E: 210                   I: 29,636                R: 270.0                 S: 1.0                   T: 1:22:04            \n",
            " train            E: 211                   I: 29,827                R: 860.0                 S: 1.0                   T: 1:22:25            \n",
            " train            E: 212                   I: 29,963                R: 460.0                 S: 1.0                   T: 1:22:41            \n",
            " eval             E: 213                   I: 30,113                R: 580.0                 S: 1.0                   T: 1:23:44            \n",
            " train            E: 213                   I: 30,113                R: 570.0                 S: 1.0                   T: 1:23:44            \n",
            " train            E: 214                   I: 30,262                R: 410.0                 S: 1.0                   T: 1:24:01            \n",
            " train            E: 215                   I: 30,607                R: 1290.0                S: 1.0                   T: 1:24:39            \n",
            " train            E: 216                   I: 30,720                R: 370.0                 S: 1.0                   T: 1:24:52            \n",
            " train            E: 217                   I: 30,903                R: 580.0                 S: 1.0                   T: 1:25:13            \n",
            " eval             E: 218                   I: 31,050                R: 530.0                 S: 1.0                   T: 1:26:19            \n",
            " train            E: 218                   I: 31,050                R: 400.0                 S: 1.0                   T: 1:26:19            \n",
            " train            E: 219                   I: 31,180                R: 360.0                 S: 1.0                   T: 1:26:34            \n",
            " train            E: 220                   I: 31,320                R: 540.0                 S: 1.0                   T: 1:26:50            \n",
            " train            E: 221                   I: 31,426                R: 290.0                 S: 1.0                   T: 1:27:01            \n",
            " train            E: 222                   I: 31,553                R: 400.0                 S: 1.0                   T: 1:27:15            \n",
            " train            E: 223                   I: 31,693                R: 440.0                 S: 1.0                   T: 1:27:31            \n",
            " train            E: 224                   I: 31,812                R: 380.0                 S: 1.0                   T: 1:27:44            \n",
            " eval             E: 225                   I: 32,016                R: 290.0                 S: 1.0                   T: 1:28:54            \n",
            " train            E: 225                   I: 32,016                R: 2160.0                S: 1.0                   T: 1:28:54            \n",
            " train            E: 226                   I: 32,182                R: 740.0                 S: 1.0                   T: 1:29:12            \n",
            " train            E: 227                   I: 32,309                R: 400.0                 S: 1.0                   T: 1:29:26            \n",
            " train            E: 228                   I: 32,470                R: 630.0                 S: 1.0                   T: 1:29:45            \n",
            " train            E: 229                   I: 32,594                R: 350.0                 S: 1.0                   T: 1:30:01            \n",
            " train            E: 230                   I: 32,707                R: 410.0                 S: 1.0                   T: 1:30:14            \n",
            " train            E: 231                   I: 32,838                R: 360.0                 S: 1.0                   T: 1:30:28            \n",
            " train            E: 232                   I: 32,921                R: 360.0                 S: 1.0                   T: 1:30:37            \n",
            " eval             E: 233                   I: 33,119                R: 330.0                 S: 1.0                   T: 1:31:50            \n",
            " train            E: 233                   I: 33,119                R: 760.0                 S: 1.0                   T: 1:31:50            \n",
            " train            E: 234                   I: 33,236                R: 350.0                 S: 1.0                   T: 1:32:03            \n",
            " train            E: 235                   I: 33,341                R: 440.0                 S: 1.0                   T: 1:32:14            \n",
            " train            E: 236                   I: 33,464                R: 430.0                 S: 1.0                   T: 1:32:27            \n",
            " train            E: 237                   I: 33,607                R: 300.0                 S: 1.0                   T: 1:32:43            \n",
            " train            E: 238                   I: 33,704                R: 250.0                 S: 1.0                   T: 1:32:54            \n",
            " train            E: 239                   I: 33,843                R: 520.0                 S: 1.0                   T: 1:33:09            \n",
            " eval             E: 240                   I: 34,041                R: 330.0                 S: 1.0                   T: 1:34:17            \n",
            " train            E: 240                   I: 34,041                R: 910.0                 S: 1.0                   T: 1:34:17            \n",
            " train            E: 241                   I: 34,213                R: 900.0                 S: 1.0                   T: 1:34:36            \n",
            " train            E: 242                   I: 34,427                R: 1170.0                S: 1.0                   T: 1:35:00            \n",
            " train            E: 243                   I: 34,552                R: 470.0                 S: 1.0                   T: 1:35:15            \n",
            " train            E: 244                   I: 34,738                R: 850.0                 S: 1.0                   T: 1:35:37            \n",
            " train            E: 245                   I: 34,874                R: 420.0                 S: 1.0                   T: 1:35:52            \n",
            " eval             E: 246                   I: 35,028                R: 800.0                 S: 1.0                   T: 1:37:01            \n",
            " train            E: 246                   I: 35,028                R: 460.0                 S: 1.0                   T: 1:37:01            \n",
            " train            E: 247                   I: 35,185                R: 770.0                 S: 1.0                   T: 1:37:19            \n",
            " train            E: 248                   I: 35,299                R: 370.0                 S: 1.0                   T: 1:37:31            \n",
            " train            E: 249                   I: 35,424                R: 290.0                 S: 1.0                   T: 1:37:45            \n",
            " train            E: 250                   I: 35,529                R: 350.0                 S: 1.0                   T: 1:37:56            \n",
            " train            E: 251                   I: 35,631                R: 270.0                 S: 1.0                   T: 1:38:08            \n",
            " train            E: 252                   I: 35,813                R: 2060.0                S: 1.0                   T: 1:38:28            \n",
            " train            E: 253                   I: 35,968                R: 1150.0                S: 1.0                   T: 1:38:45            \n",
            " eval             E: 254                   I: 36,103                R: 350.0                 S: 1.0                   T: 1:39:48            \n",
            " train            E: 254                   I: 36,103                R: 500.0                 S: 1.0                   T: 1:39:48            \n",
            " train            E: 255                   I: 36,203                R: 470.0                 S: 1.0                   T: 1:39:59            \n",
            " train            E: 256                   I: 36,293                R: 300.0                 S: 1.0                   T: 1:40:09            \n",
            " train            E: 257                   I: 36,430                R: 450.0                 S: 1.0                   T: 1:40:24            \n",
            " train            E: 258                   I: 36,605                R: 470.0                 S: 1.0                   T: 1:40:42            \n",
            " train            E: 259                   I: 36,724                R: 300.0                 S: 1.0                   T: 1:40:55            \n",
            " train            E: 260                   I: 36,836                R: 370.0                 S: 1.0                   T: 1:41:08            \n",
            " eval             E: 261                   I: 37,029                R: 2060.0                S: 1.0                   T: 1:42:18            \n",
            " train            E: 261                   I: 37,029                R: 630.0                 S: 1.0                   T: 1:42:18            \n",
            " train            E: 262                   I: 37,194                R: 760.0                 S: 1.0                   T: 1:42:38            \n",
            " train            E: 263                   I: 37,288                R: 200.0                 S: 1.0                   T: 1:42:49            \n",
            " train            E: 264                   I: 37,492                R: 1440.0                S: 1.0                   T: 1:43:13            \n",
            " train            E: 265                   I: 37,603                R: 450.0                 S: 1.0                   T: 1:43:25            \n",
            " train            E: 266                   I: 37,767                R: 610.0                 S: 1.0                   T: 1:43:46            \n",
            " train            E: 267                   I: 37,885                R: 330.0                 S: 1.0                   T: 1:44:00            \n",
            " eval             E: 268                   I: 38,017                R: 390.0                 S: 1.0                   T: 1:45:05            \n",
            " train            E: 268                   I: 38,017                R: 320.0                 S: 1.0                   T: 1:45:05            \n",
            " train            E: 269                   I: 38,200                R: 920.0                 S: 1.0                   T: 1:45:28            \n",
            " train            E: 270                   I: 38,317                R: 430.0                 S: 1.0                   T: 1:45:42            \n",
            " train            E: 271                   I: 38,441                R: 330.0                 S: 1.0                   T: 1:45:57            \n",
            " train            E: 272                   I: 38,569                R: 440.0                 S: 1.0                   T: 1:46:13            \n",
            " train            E: 273                   I: 38,724                R: 1270.0                S: 1.0                   T: 1:46:32            \n",
            " train            E: 274                   I: 38,808                R: 230.0                 S: 1.0                   T: 1:46:42            \n",
            " train            E: 275                   I: 38,968                R: 710.0                 S: 1.0                   T: 1:47:01            \n",
            " eval             E: 276                   I: 39,163                R: 850.0                 S: 1.0                   T: 1:48:17            \n",
            " train            E: 276                   I: 39,163                R: 1180.0                S: 1.0                   T: 1:48:17            \n",
            " train            E: 277                   I: 39,286                R: 420.0                 S: 1.0                   T: 1:48:32            \n",
            " train            E: 278                   I: 39,469                R: 990.0                 S: 1.0                   T: 1:48:54            \n",
            " train            E: 279                   I: 39,631                R: 620.0                 S: 1.0                   T: 1:49:13            \n",
            " train            E: 280                   I: 39,832                R: 1360.0                S: 1.0                   T: 1:49:37            \n",
            " train            E: 281                   I: 39,964                R: 380.0                 S: 1.0                   T: 1:49:52            \n",
            " eval             E: 282                   I: 40,120                R: 350.0                 S: 1.0                   T: 1:50:58            \n",
            " train            E: 282                   I: 40,120                R: 590.0                 S: 1.0                   T: 1:50:58            \n",
            " train            E: 283                   I: 40,240                R: 360.0                 S: 1.0                   T: 1:51:11            \n",
            " train            E: 284                   I: 40,364                R: 420.0                 S: 1.0                   T: 1:51:25            \n",
            " train            E: 285                   I: 40,510                R: 470.0                 S: 1.0                   T: 1:51:41            \n",
            " train            E: 286                   I: 40,658                R: 570.0                 S: 1.0                   T: 1:51:58            \n",
            " train            E: 287                   I: 40,792                R: 470.0                 S: 1.0                   T: 1:52:14            \n",
            " train            E: 288                   I: 40,900                R: 310.0                 S: 1.0                   T: 1:52:27            \n",
            " eval             E: 289                   I: 41,032                R: 400.0                 S: 1.0                   T: 1:53:32            \n",
            " train            E: 289                   I: 41,032                R: 460.0                 S: 1.0                   T: 1:53:32            \n",
            " train            E: 290                   I: 41,138                R: 270.0                 S: 1.0                   T: 1:53:44            \n",
            " train            E: 291                   I: 41,263                R: 390.0                 S: 1.0                   T: 1:53:59            \n",
            " train            E: 292                   I: 41,451                R: 620.0                 S: 1.0                   T: 1:54:21            \n",
            " train            E: 293                   I: 41,552                R: 360.0                 S: 1.0                   T: 1:54:33            \n",
            " train            E: 294                   I: 41,718                R: 890.0                 S: 1.0                   T: 1:54:53            \n",
            " train            E: 295                   I: 41,801                R: 270.0                 S: 1.0                   T: 1:55:02            \n",
            " eval             E: 296                   I: 42,067                R: 320.0                 S: 1.0                   T: 1:56:19            \n",
            " train            E: 296                   I: 42,067                R: 1980.0                S: 1.0                   T: 1:56:19            \n",
            " train            E: 297                   I: 42,179                R: 350.0                 S: 1.0                   T: 1:56:32            \n",
            " train            E: 298                   I: 42,284                R: 310.0                 S: 1.0                   T: 1:56:44            \n",
            " train            E: 299                   I: 42,472                R: 1010.0                S: 1.0                   T: 1:57:05            \n",
            " train            E: 300                   I: 42,631                R: 520.0                 S: 1.0                   T: 1:57:23            \n",
            " train            E: 301                   I: 42,773                R: 520.0                 S: 1.0                   T: 1:57:39            \n",
            " train            E: 302                   I: 42,962                R: 900.0                 S: 1.0                   T: 1:58:01            \n",
            " eval             E: 303                   I: 43,176                R: 900.0                 S: 1.0                   T: 1:59:21            \n",
            " train            E: 303                   I: 43,176                R: 1670.0                S: 1.0                   T: 1:59:21            \n",
            " train            E: 304                   I: 43,292                R: 430.0                 S: 1.0                   T: 1:59:34            \n",
            " train            E: 305                   I: 43,408                R: 510.0                 S: 1.0                   T: 1:59:47            \n",
            " train            E: 306                   I: 43,504                R: 220.0                 S: 1.0                   T: 1:59:58            \n",
            " train            E: 307                   I: 43,628                R: 470.0                 S: 1.0                   T: 2:00:12            \n",
            " train            E: 308                   I: 43,771                R: 430.0                 S: 1.0                   T: 2:00:28            \n",
            " train            E: 309                   I: 43,907                R: 380.0                 S: 1.0                   T: 2:00:44            \n",
            " eval             E: 310                   I: 44,104                R: 460.0                 S: 1.0                   T: 2:01:54            \n",
            " train            E: 310                   I: 44,104                R: 2070.0                S: 1.0                   T: 2:01:54            \n",
            " train            E: 311                   I: 44,262                R: 640.0                 S: 1.0                   T: 2:02:12            \n",
            " train            E: 312                   I: 44,385                R: 330.0                 S: 1.0                   T: 2:02:26            \n",
            " train            E: 313                   I: 44,612                R: 1160.0                S: 1.0                   T: 2:02:51            \n",
            " train            E: 314                   I: 44,748                R: 590.0                 S: 1.0                   T: 2:03:06            \n",
            " train            E: 315                   I: 44,816                R: 180.0                 S: 1.0                   T: 2:03:14            \n",
            " train            E: 316                   I: 44,985                R: 460.0                 S: 1.0                   T: 2:03:33            \n",
            " eval             E: 317                   I: 45,184                R: 440.0                 S: 1.0                   T: 2:04:44            \n",
            " train            E: 317                   I: 45,184                R: 740.0                 S: 1.0                   T: 2:04:44            \n",
            " train            E: 318                   I: 45,291                R: 380.0                 S: 1.0                   T: 2:04:56            \n",
            " train            E: 319                   I: 45,465                R: 530.0                 S: 1.0                   T: 2:05:16            \n",
            " train            E: 320                   I: 45,591                R: 390.0                 S: 1.0                   T: 2:05:30            \n",
            " train            E: 321                   I: 45,748                R: 650.0                 S: 1.0                   T: 2:05:47            \n",
            " train            E: 322                   I: 45,851                R: 360.0                 S: 1.0                   T: 2:05:59            \n",
            " eval             E: 323                   I: 46,015                R: 400.0                 S: 1.0                   T: 2:07:03            \n",
            " train            E: 323                   I: 46,015                R: 910.0                 S: 1.0                   T: 2:07:03            \n",
            " train            E: 324                   I: 46,145                R: 300.0                 S: 1.0                   T: 2:07:18            \n",
            " train            E: 325                   I: 46,247                R: 290.0                 S: 1.0                   T: 2:07:29            \n",
            " train            E: 326                   I: 46,377                R: 380.0                 S: 1.0                   T: 2:07:44            \n",
            " train            E: 327                   I: 46,573                R: 1130.0                S: 1.0                   T: 2:08:06            \n",
            " train            E: 328                   I: 46,665                R: 360.0                 S: 1.0                   T: 2:08:16            \n",
            " train            E: 329                   I: 46,764                R: 380.0                 S: 1.0                   T: 2:08:27            \n",
            " train            E: 330                   I: 46,886                R: 320.0                 S: 1.0                   T: 2:08:42            \n",
            " eval             E: 331                   I: 47,052                R: 580.0                 S: 1.0                   T: 2:09:51            \n",
            " train            E: 331                   I: 47,052                R: 670.0                 S: 1.0                   T: 2:09:51            \n",
            " train            E: 332                   I: 47,165                R: 430.0                 S: 1.0                   T: 2:10:04            \n",
            " train            E: 333                   I: 47,257                R: 280.0                 S: 1.0                   T: 2:10:15            \n",
            " train            E: 334                   I: 47,383                R: 400.0                 S: 1.0                   T: 2:10:29            \n",
            " train            E: 335                   I: 47,497                R: 380.0                 S: 1.0                   T: 2:10:41            \n",
            " train            E: 336                   I: 47,616                R: 340.0                 S: 1.0                   T: 2:10:54            \n",
            " train            E: 337                   I: 47,737                R: 330.0                 S: 1.0                   T: 2:11:08            \n",
            " train            E: 338                   I: 47,878                R: 510.0                 S: 1.0                   T: 2:11:24            \n",
            " eval             E: 339                   I: 48,030                R: 320.0                 S: 1.0                   T: 2:12:27            \n",
            " train            E: 339                   I: 48,030                R: 530.0                 S: 1.0                   T: 2:12:27            \n",
            " train            E: 340                   I: 48,184                R: 680.0                 S: 1.0                   T: 2:12:45            \n",
            " train            E: 341                   I: 48,362                R: 740.0                 S: 1.0                   T: 2:13:05            \n",
            " train            E: 342                   I: 48,477                R: 250.0                 S: 1.0                   T: 2:13:18            \n",
            " train            E: 343                   I: 48,595                R: 240.0                 S: 1.0                   T: 2:13:32            \n",
            " train            E: 344                   I: 48,707                R: 380.0                 S: 1.0                   T: 2:13:45            \n",
            " train            E: 345                   I: 48,894                R: 610.0                 S: 1.0                   T: 2:14:05            \n",
            " eval             E: 346                   I: 49,023                R: 220.0                 S: 1.0                   T: 2:15:05            \n",
            " train            E: 346                   I: 49,023                R: 320.0                 S: 1.0                   T: 2:15:05            \n",
            " train            E: 347                   I: 49,126                R: 360.0                 S: 1.0                   T: 2:15:17            \n",
            " train            E: 348                   I: 49,247                R: 430.0                 S: 1.0                   T: 2:15:30            \n",
            " train            E: 349                   I: 49,351                R: 340.0                 S: 1.0                   T: 2:15:42            \n",
            " train            E: 350                   I: 49,443                R: 310.0                 S: 1.0                   T: 2:15:52            \n",
            " train            E: 351                   I: 49,612                R: 490.0                 S: 1.0                   T: 2:16:10            \n",
            " train            E: 352                   I: 49,761                R: 440.0                 S: 1.0                   T: 2:16:27            \n",
            " train            E: 353                   I: 49,892                R: 390.0                 S: 1.0                   T: 2:16:41            \n",
            " eval             E: 354                   I: 50,046                R: 300.0                 S: 1.0                   T: 2:17:44            \n",
            " train            E: 354                   I: 50,046                R: 660.0                 S: 1.0                   T: 2:17:44            \n",
            " train            E: 355                   I: 50,230                R: 790.0                 S: 1.0                   T: 2:18:04            \n",
            " train            E: 356                   I: 50,343                R: 350.0                 S: 1.0                   T: 2:18:17            \n",
            " train            E: 357                   I: 50,554                R: 1230.0                S: 1.0                   T: 2:18:40            \n",
            " train            E: 358                   I: 50,666                R: 430.0                 S: 1.0                   T: 2:18:52            \n",
            " eval             E: 359                   I: 51,044                R: 290.0                 S: 1.0                   T: 2:20:18            \n",
            " train            E: 359                   I: 51,044                R: 1340.0                S: 1.0                   T: 2:20:18            \n",
            " train            E: 360                   I: 51,171                R: 530.0                 S: 1.0                   T: 2:20:32            \n",
            " train            E: 361                   I: 51,314                R: 540.0                 S: 1.0                   T: 2:20:47            \n",
            " train            E: 362                   I: 51,436                R: 310.0                 S: 1.0                   T: 2:21:01            \n",
            " train            E: 363                   I: 51,586                R: 520.0                 S: 1.0                   T: 2:21:17            \n",
            " train            E: 364                   I: 51,774                R: 650.0                 S: 1.0                   T: 2:21:38            \n",
            " train            E: 365                   I: 51,933                R: 880.0                 S: 1.0                   T: 2:21:55            \n",
            " eval             E: 366                   I: 52,085                R: 380.0                 S: 1.0                   T: 2:22:58            \n",
            " train            E: 366                   I: 52,085                R: 670.0                 S: 1.0                   T: 2:22:58            \n",
            " train            E: 367                   I: 52,245                R: 750.0                 S: 1.0                   T: 2:23:16            \n",
            " train            E: 368                   I: 52,409                R: 660.0                 S: 1.0                   T: 2:23:34            \n",
            " train            E: 369                   I: 52,689                R: 690.0                 S: 1.0                   T: 2:24:08            \n",
            " train            E: 370                   I: 52,851                R: 810.0                 S: 1.0                   T: 2:24:26            \n",
            " eval             E: 371                   I: 53,002                R: 320.0                 S: 1.0                   T: 2:25:33            \n",
            " train            E: 371                   I: 53,002                R: 550.0                 S: 1.0                   T: 2:25:33            \n",
            " train            E: 372                   I: 53,113                R: 390.0                 S: 1.0                   T: 2:25:46            \n",
            " train            E: 373                   I: 53,196                R: 310.0                 S: 1.0                   T: 2:25:56            \n",
            " train            E: 374                   I: 53,341                R: 470.0                 S: 1.0                   T: 2:26:13            \n",
            " train            E: 375                   I: 53,547                R: 1160.0                S: 1.0                   T: 2:26:40            \n",
            " train            E: 376                   I: 53,690                R: 420.0                 S: 1.0                   T: 2:26:59            \n",
            " train            E: 377                   I: 53,874                R: 830.0                 S: 1.0                   T: 2:27:22            \n",
            " eval             E: 378                   I: 54,006                R: 1110.0                S: 1.0                   T: 2:28:33            \n",
            " train            E: 378                   I: 54,006                R: 470.0                 S: 1.0                   T: 2:28:33            \n",
            " train            E: 379                   I: 54,122                R: 340.0                 S: 1.0                   T: 2:28:47            \n",
            " train            E: 380                   I: 54,290                R: 740.0                 S: 1.0                   T: 2:29:08            \n",
            " train            E: 381                   I: 54,391                R: 330.0                 S: 1.0                   T: 2:29:19            \n",
            " train            E: 382                   I: 54,535                R: 550.0                 S: 1.0                   T: 2:29:38            \n",
            " train            E: 383                   I: 54,674                R: 630.0                 S: 1.0                   T: 2:29:55            \n",
            " train            E: 384                   I: 54,778                R: 250.0                 S: 1.0                   T: 2:30:07            \n",
            " train            E: 385                   I: 54,949                R: 730.0                 S: 1.0                   T: 2:30:27            \n",
            " eval             E: 386                   I: 55,157                R: 590.0                 S: 1.0                   T: 2:31:46            \n",
            " train            E: 386                   I: 55,157                R: 2630.0                S: 1.0                   T: 2:31:46            \n",
            " train            E: 387                   I: 55,271                R: 380.0                 S: 1.0                   T: 2:32:00            \n",
            " train            E: 388                   I: 55,423                R: 530.0                 S: 1.0                   T: 2:32:20            \n",
            " train            E: 389                   I: 55,590                R: 820.0                 S: 1.0                   T: 2:32:40            \n",
            " train            E: 390                   I: 55,720                R: 390.0                 S: 1.0                   T: 2:32:56            \n",
            " train            E: 391                   I: 55,838                R: 240.0                 S: 1.0                   T: 2:33:11            \n",
            " eval             E: 392                   I: 56,063                R: 510.0                 S: 1.0                   T: 2:34:28            \n",
            " train            E: 392                   I: 56,063                R: 800.0                 S: 1.0                   T: 2:34:28            \n",
            " train            E: 393                   I: 56,287                R: 1080.0                S: 1.0                   T: 2:34:54            \n",
            " train            E: 394                   I: 56,398                R: 320.0                 S: 1.0                   T: 2:35:07            \n",
            " train            E: 395                   I: 56,531                R: 430.0                 S: 1.0                   T: 2:35:22            \n",
            " train            E: 396                   I: 56,659                R: 500.0                 S: 1.0                   T: 2:35:37            \n",
            " train            E: 397                   I: 56,843                R: 560.0                 S: 1.0                   T: 2:35:58            \n",
            " eval             E: 398                   I: 57,016                R: 400.0                 S: 1.0                   T: 2:37:07            \n",
            " train            E: 398                   I: 57,016                R: 520.0                 S: 1.0                   T: 2:37:07            \n",
            " train            E: 399                   I: 57,129                R: 360.0                 S: 1.0                   T: 2:37:20            \n",
            " train            E: 400                   I: 57,210                R: 250.0                 S: 1.0                   T: 2:37:30            \n",
            " train            E: 401                   I: 57,325                R: 360.0                 S: 1.0                   T: 2:37:43            \n",
            " train            E: 402                   I: 57,432                R: 360.0                 S: 1.0                   T: 2:37:56            \n",
            " train            E: 403                   I: 57,578                R: 520.0                 S: 1.0                   T: 2:38:13            \n",
            " train            E: 404                   I: 57,683                R: 330.0                 S: 1.0                   T: 2:38:25            \n",
            " train            E: 405                   I: 57,791                R: 280.0                 S: 1.0                   T: 2:38:38            \n",
            " train            E: 406                   I: 57,893                R: 300.0                 S: 1.0                   T: 2:38:50            \n",
            " eval             E: 407                   I: 58,049                R: 340.0                 S: 1.0                   T: 2:39:58            \n",
            " train            E: 407                   I: 58,049                R: 810.0                 S: 1.0                   T: 2:39:58            \n",
            " train            E: 408                   I: 58,183                R: 470.0                 S: 1.0                   T: 2:40:15            \n",
            " train            E: 409                   I: 58,336                R: 640.0                 S: 1.0                   T: 2:40:33            \n",
            " train            E: 410                   I: 58,446                R: 350.0                 S: 1.0                   T: 2:40:46            \n",
            " train            E: 411                   I: 58,608                R: 870.0                 S: 1.0                   T: 2:41:05            \n",
            " train            E: 412                   I: 58,708                R: 350.0                 S: 1.0                   T: 2:41:16            \n",
            " train            E: 413                   I: 58,822                R: 260.0                 S: 1.0                   T: 2:41:29            \n",
            " train            E: 414                   I: 58,921                R: 260.0                 S: 1.0                   T: 2:41:40            \n",
            " eval             E: 415                   I: 59,086                R: 560.0                 S: 1.0                   T: 2:42:46            \n",
            " train            E: 415                   I: 59,086                R: 600.0                 S: 1.0                   T: 2:42:46            \n",
            " train            E: 416                   I: 59,181                R: 370.0                 S: 1.0                   T: 2:42:57            \n",
            " train            E: 417                   I: 59,390                R: 930.0                 S: 1.0                   T: 2:43:21            \n",
            " train            E: 418                   I: 59,506                R: 280.0                 S: 1.0                   T: 2:43:34            \n",
            " train            E: 419                   I: 59,698                R: 510.0                 S: 1.0                   T: 2:43:56            \n",
            " train            E: 420                   I: 59,797                R: 300.0                 S: 1.0                   T: 2:44:08            \n",
            " train            E: 421                   I: 59,983                R: 1300.0                S: 1.0                   T: 2:44:29            \n",
            " eval             E: 422                   I: 60,072                R: 150.0                 S: 1.0                   T: 2:45:24            \n",
            " train            E: 422                   I: 60,072                R: 330.0                 S: 1.0                   T: 2:45:24            \n",
            " train            E: 423                   I: 60,225                R: 420.0                 S: 1.0                   T: 2:45:42            \n",
            " train            E: 424                   I: 60,426                R: 1460.0                S: 1.0                   T: 2:46:05            \n",
            " train            E: 425                   I: 60,542                R: 280.0                 S: 1.0                   T: 2:46:19            \n",
            " train            E: 426                   I: 60,652                R: 300.0                 S: 1.0                   T: 2:46:31            \n",
            " train            E: 427                   I: 60,733                R: 270.0                 S: 1.0                   T: 2:46:41            \n",
            " train            E: 428                   I: 60,960                R: 970.0                 S: 1.0                   T: 2:47:07            \n",
            " eval             E: 429                   I: 61,050                R: 550.0                 S: 1.0                   T: 2:48:08            \n",
            " train            E: 429                   I: 61,050                R: 360.0                 S: 1.0                   T: 2:48:08            \n",
            " train            E: 430                   I: 61,257                R: 2130.0                S: 1.0                   T: 2:48:33            \n",
            " train            E: 431                   I: 61,386                R: 490.0                 S: 1.0                   T: 2:48:49            \n",
            " train            E: 432                   I: 61,522                R: 390.0                 S: 1.0                   T: 2:49:06            \n",
            " train            E: 433                   I: 61,679                R: 830.0                 S: 1.0                   T: 2:49:25            \n"
          ]
        }
      ],
      "source": [
        "# 学習\n",
        "assert torch.cuda.is_available()\n",
        "assert cfg.steps > 0, \"Must train for at least 1 step.\"\n",
        "print(\"Work dir:\", cfg.work_dir)\n",
        "\n",
        "trainer = OnlineTrainer(\n",
        "    cfg=cfg,\n",
        "    env=env,\n",
        "    eval_env=eval_env,\n",
        "    agent=TDMPC2(cfg),\n",
        "    buffer=Buffer(cfg),\n",
        "    logger=Logger(cfg),\n",
        ")\n",
        "trainer.train()\n",
        "print(\"\\nTraining completed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f17fa5",
      "metadata": {},
      "source": [
        "これ以下はいったん無視"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa693a51-a4cb-4ad4-be2b-322cbd68443d",
      "metadata": {
        "id": "aa693a51-a4cb-4ad4-be2b-322cbd68443d"
      },
      "source": [
        "## 6. モデルの保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd4373bf-b090-4ab9-9736-05bf9ba84ef3",
      "metadata": {
        "id": "bd4373bf-b090-4ab9-9736-05bf9ba84ef3"
      },
      "outputs": [],
      "source": [
        "# モデルの保存(Google Driveの場合）\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "trained_models.save(\"drive/MyDrive/Colab Notebooks/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4b31352-bafa-46ed-8bcc-632a24dfced6",
      "metadata": {
        "id": "c4b31352-bafa-46ed-8bcc-632a24dfced6"
      },
      "source": [
        "## 7. 学習済みパラメータで評価  \n",
        "- こちらの評価に用いている環境は，Omnicampus上で評価する際に用いる環境と同じになっています．\n",
        "- 今回のコンペティションではPublic / Privateの分類はないため，基本的には以下の実装の評価を性能の目安としていただくと良いと思います．  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7aa98bd-a9da-4223-9341-0f3cb489211e",
      "metadata": {
        "id": "c7aa98bd-a9da-4223-9341-0f3cb489211e"
      },
      "outputs": [],
      "source": [
        "# 環境の読み込み\n",
        "env = make_env()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 学習済みモデルの読み込み\n",
        "rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "\n",
        "trained_models = TrainedModels(\n",
        "    rssm,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    reward_model,\n",
        "    discount_model,\n",
        "    actor,\n",
        "    critic\n",
        ")\n",
        "\n",
        "trained_models.load(\"./\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a745c7-62dd-4431-99de-4da0f1489a16",
      "metadata": {
        "id": "41a745c7-62dd-4431-99de-4da0f1489a16"
      },
      "outputs": [],
      "source": [
        "# 結果を動画で観てみるための関数\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    plt.figure(figsize=(8, 8), dpi=50)\n",
        "    patch = plt.imshow(frames[0], cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "        plt.title(\"Step %d\" % (i))\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "    display(HTML(anim.to_jshtml(default_mode='once')))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e365b2-bc62-4fe9-a9d9-a94e42d382f2",
      "metadata": {
        "id": "12e365b2-bc62-4fe9-a9d9-a94e42d382f2"
      },
      "source": [
        "**環境のシードを固定して評価を行います．シードを変更しないでください．**\n",
        "- 変更した場合，Omnicampus上での評価と結果が異なります．  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206c8746-7371-4142-bd92-dde301c2f33c",
      "metadata": {
        "id": "206c8746-7371-4142-bd92-dde301c2f33c"
      },
      "outputs": [],
      "source": [
        "env = make_env(seed=1234, max_steps=None)\n",
        "\n",
        "policy = Agent(encoder, rssm, actor)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = [obs]\n",
        "actions = []\n",
        "\n",
        "while not done:\n",
        "    action = policy(obs, eval=True)\n",
        "    action_int = np.argmax(action)  # 環境に渡すときはint型\n",
        "\n",
        "    obs, reward, done, _ = env.step(action_int)\n",
        "\n",
        "    total_reward += reward\n",
        "    frames.append(obs)\n",
        "    actions.append(action_int)\n",
        "\n",
        "print('Total Reward:', total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11353d5f-2551-4d45-9ea0-a1f9ab708dd0",
      "metadata": {
        "id": "11353d5f-2551-4d45-9ea0-a1f9ab708dd0"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d1b91f-34b5-48ec-9082-0ded1007952c",
      "metadata": {
        "id": "f8d1b91f-34b5-48ec-9082-0ded1007952c"
      },
      "source": [
        "今回，評価を行う際のrepeat actionは1に設定しています．  \n",
        "そのため，repeat actionをそれ以外に設定している場合，repeat actionの分だけ繰り返した行動を提出する形にしています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e850a3d-824a-490a-b12b-9b9ef2100c3d",
      "metadata": {
        "id": "0e850a3d-824a-490a-b12b-9b9ef2100c3d"
      },
      "outputs": [],
      "source": [
        "# repeat actionに対応した行動に変換する\n",
        "submission_actions = np.zeros(len(actions) * env._skip)\n",
        "for start_idx in range(env._skip):\n",
        "    submission_actions[start_idx::env._skip] = np.array(actions)\n",
        "\n",
        "np.save(\"drive/MyDrive/submission\", submission_actions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
